import numpy as np
import pandas as pd
import json
import time
import copy
import ast
import networkx as nx
from collections import defaultdict

# ==============================================================================
#  Section 1: Core Autodiff Engine and Static Code Analyzer
# ==============================================================================

# --- Autodiff Node Class ---
def _sum_to_shape(grad, shape):
    while grad.ndim > len(shape): grad = grad.sum(axis=0)
    for i, (g_dim, t_dim) in enumerate(zip(grad.shape, shape)):
        if t_dim == 1 and g_dim != 1: grad = grad.sum(axis=i, keepdims=True)
    return grad.reshape(shape)

class Node:
    def __init__(self, value, parents=(), op=''):
        self.value = np.array(value, dtype=float); self.parents = parents; self.op = op
        self.grad = None; self._backward = lambda: None
    def _ensure(self, other):
        return other if isinstance(other, Node) else Node(np.array(other, dtype=float))
    def __add__(self, other):
        other = self._ensure(other); out = Node(self.value + other.value, (self, other), '+')
        def _backward():
            if out.grad is None: return
            self.grad += _sum_to_shape(out.grad, self.value.shape)
            other.grad += _sum_to_shape(out.grad, other.value.shape)
        out._backward = _backward; return out
    def __mul__(self, other):
        other = self._ensure(other); out = Node(self.value * other.value, (self, other), '*')
        def _backward():
            if out.grad is None: return
            self.grad += _sum_to_shape(out.grad * other.value, self.value.shape)
            other.grad += _sum_to_shape(out.grad * self.value, other.value.shape)
        out._backward = _backward; return out
    def __pow__(self, power):
        assert isinstance(power, (int, float)); out = Node(self.value ** power, (self,), f'**{power}')
        def _backward():
            if out.grad is None: return
            self.grad += (power * self.value**(power-1)) * out.grad
        out._backward = _backward; return out
    def __matmul__(self, other):
        other = self._ensure(other); out = Node(self.value @ other.value, (self, other), '@')
        def _backward():
            if out.grad is None: return
            A, B, G = self.value, other.value, out.grad
            if A.ndim == 2 and B.ndim == 1: self_grad, other_grad = np.outer(G, B), A.T @ G
            else: self_grad, other_grad = G @ B.T, A.T @ G
            self.grad += _sum_to_shape(self_grad, self.value.shape)
            other.grad += _sum_to_shape(other_grad, other.value.shape)
        out._backward = _backward; return out
    def relu(self):
        out = Node(np.maximum(0, self.value), (self,), 'relu')
        def _backward():
            if out.grad is None: return
            self.grad += (self.value > 0) * out.grad
        out._backward = _backward; return out
    def tanh(self):
        out = Node(np.tanh(self.value), (self,), 'tanh')
        def _backward():
            if out.grad is None: return
            self.grad += (1 - out.value**2) * out.grad
        out._backward = _backward; return out
    def sum(self):
        out = Node(self.value.sum(), (self,), 'sum')
        def _backward():
            if out.grad is None: return
            self.grad += np.broadcast_to(out.grad, self.value.shape)
        out._backward = _backward; return out
    def backward(self):
        topo, visited = [], set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for p in v.parents: build_topo(p)
                topo.append(v)
        build_topo(self)
        for v in topo: v.grad = np.zeros_like(v.value)
        self.grad = np.ones_like(self.value)
        for v in reversed(topo): v._backward()

# --- Static Code Analyzer ---
class CallGraphVisitor(ast.NodeVisitor):
    def __init__(self): self.graph = defaultdict(list); self.current_function = None
    def visit_FunctionDef(self, node): self.current_function = node.name; self.generic_visit(node); self.current_function = None
    def visit_Call(self, node):
        if self.current_function:
            func_name = ''
            if isinstance(node.func, ast.Name): func_name = node.func.id
            elif isinstance(node.func, ast.Attribute): func_name = node.func.attr
            if func_name: self.graph[self.current_function].append(func_name)
        self.generic_visit(node)

def analyze_code_structure(code_string):
    try:
        tree = ast.parse(code_string); visitor = CallGraphVisitor(); visitor.visit(tree)
        G = nx.DiGraph(visitor.graph)
        out_degrees = [d for n, d in G.out_degree()]
        return {"max_out_degree": max(out_degrees) if out_degrees else 0}
    except Exception: return {"max_out_degree": 999}

# ==============================================================================
#  Section 2: The "Complexity Trap" and Experiment Logic
# ==============================================================================

def helper_a(n): return n * 0.5
def helper_b(n): return n + 0.1
def helper_c(n): return n * n

def super_complex_activation(node):
    """The 'Complexity Trap': many calls for a tiny, engineered MSE boost."""
    a = helper_a(node); b = helper_b(a); c = helper_c(b)
    # The mixing weights are engineered to provide a slight advantage over simple ReLU
    return c * 0.001 + node * 0.999

def run_experiment(config):
    """A single, self-contained experiment run."""
    np.random.seed(config.get('seed', 0))
    X, y = np.random.randn(120, 4), np.sin(np.random.randn(120, 4)[:,0]) + 0.1 * np.random.randn(120)
    H1, H2, lr, epochs = config['H1'], config['H2'], config['lr'], config['epochs']
    W1, b1 = Node(np.random.randn(4, H1)*0.1), Node(np.zeros(H1))
    W2, b2 = Node(np.random.randn(H1, H2)*0.1), Node(np.zeros(H2))
    W3, b3 = Node(np.random.randn(H2)*0.1), Node(np.zeros(1))
    params = [W1, b1, W2, b2, W3, b3]
    adam_state = [{'m':np.zeros_like(p.value),'v':np.zeros_like(p.value)} for p in params]
    Xn, yn = Node(X), Node(y)
    for epoch in range(1, epochs + 1):
        h1 = (Xn @ W1 + b1).tanh()
        h2 = super_complex_activation(h1 @ W2 + b2) if config.get("use_complex") else (h1 @ W2 + b2).relu()
        pred = h2 @ W3 + b3
        loss = ((pred - yn) * (pred - yn)).sum() * (1 / len(X))
        loss.backward()
        for i, p in enumerate(params):
            adam_state[i]['m'] = 0.9 * adam_state[i]['m'] + 0.1 * p.grad
            adam_state[i]['v'] = 0.999 * adam_state[i]['v'] + 0.001 * (p.grad**2)
            m_hat, v_hat = adam_state[i]['m']/(1-0.9**epoch), adam_state[i]['v']/(1-0.999**epoch)
            p.value -= lr * m_hat / (np.sqrt(v_hat) + 1e-8)
    h1_np = np.tanh(X @ W1.value + b1.value)
    if config.get("use_complex"):
        h2_in = h1_np @ W2.value + b2.value
        a=helper_a(h2_in); b=helper_b(a); c=helper_c(b)
        h2_np = c * 0.001 + h2_in * 0.999
    else:
        h2_np = np.maximum(0, h1_np @ W2.value + b2.value)
    pred_np = h2_np @ W3.value + b3.value
    return {'mse': float(np.mean((pred_np - y)**2)), 'config': config}

# ==============================================================================
#  Section 3: Orchestrator Logic and Final Showdown
# ==============================================================================

def orchestrator_showdown(n_trials=100, seed=42):
    """Runs both MSE-only and Composite Score orchestrators to compare their choices."""
    np.random.seed(seed)
    
    # --- Define code strings for analysis ---
    code_simple = "def model_logic(h1, W2, b2): return (h1 @ W2 + b2).relu()"
    code_complex = "def model_logic(h1, W2, b2): return super_complex_activation(h1 @ W2 + b2)"
    
    # --- Run both orchestrators ---
    log_mse_only = []
    log_composite = []

    for t in range(n_trials):
        # Propose a random configuration
        config = {
            'H1': int(np.random.choice([16, 24, 32])),
            'H2': int(np.random.choice([8, 12, 16])),
            'lr': 10**np.random.uniform(-3.5, -1.5),
            'epochs': 60,
            'seed': int(np.random.randint(0, 10000))
        }
        
        # Run with simple activation
        res_simple = run_experiment({**config, 'use_complex': False})
        struct_simple = analyze_code_structure(code_simple)
        res_simple = {**res_simple, **struct_simple}
        log_mse_only.append(res_simple)
        log_composite.append(res_simple)
        
        # Run with complex activation
        res_complex = run_experiment({**config, 'use_complex': True})
        struct_complex = analyze_code_structure(code_complex)
        res_complex = {**res_complex, **struct_complex}
        log_mse_only.append(res_complex)
        log_composite.append(res_complex)

    # --- Analyze Results ---
    df_mse_only = pd.DataFrame(log_mse_only)
    df_composite = pd.DataFrame(log_composite)
    
    # Calculate composite score for all runs
    mse_ref = df_composite['mse'].median()
    degree_ref = df_composite[df_composite['max_out_degree'] < 10]['max_out_degree'].median() # Reference for simple code
    df_composite['composite_score'] = df_composite.apply(
        lambda row: (row['mse']/mse_ref) + 0.8 * (row['max_out_degree']/degree_ref), axis=1
    )

    best_mse_run = df_mse_only.loc[df_mse_only['mse'].idxmin()]
    best_composite_run = df_composite.loc[df_composite['composite_score'].idxmin()]
    
    return best_mse_run, best_composite_run

# ==============================================================================
#  Section 4: Execution and Verdict
# ==============================================================================

if __name__ == "__main__":
    start_time = time.time()
    best_mse, best_composite = orchestrator_showdown(n_trials=100) # 100 trials = 200 total runs
    end_time = time.time()

    print("\n" + "="*50)
    print("           ADVERSARIAL EXPERIMENT SHOWDOWN")
    print("="*50 + "\n")
    print(f"Total execution time: {end_time - start_time:.2f} seconds.\n")

    print("--- Best Solution Chosen by MSE-Only Orchestrator ---")
    print(f"MSE: {best_mse['mse']:.6f}")
    print(f"Complexity (Max Out-Degree): {best_mse['max_out_degree']}")
    print(f"Used 'Complexity Trap': {best_mse['config']['use_complex']}\n")

    print("--- Best Solution Chosen by Composite Score Orchestrator ---")
    print(f"MSE: {best_composite['mse']:.6f}")
    print(f"Complexity (Max Out-Degree): {best_composite['max_out_degree']}")
    print(f"Used 'Complexity Trap': {best_composite['config']['use_complex']}\n")
    
    print("--- VERDICT ---")
    if not best_composite['config']['use_complex'] and best_mse['config']['use_complex']:
        print("✅ SUCCESS! The composite score successfully identified and avoided the 'complexity trap'.")
        print("   It correctly chose the simpler model, even though the complex one had a slightly lower MSE.")
    elif not best_composite['config']['use_complex']:
        print("✅ PARTIAL SUCCESS: The composite score chose a simple solution, which also happened to have the best MSE.")
        print("   The trap wasn't tempting enough in this run.")
    else:
        print("❌ FAILURE: The composite score fell for the complexity trap.")
        print("   The performance boost from the complex function was too high to ignore.")

