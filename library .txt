import numpy as np
import time
from collections import OrderedDict

# ==============================================================================
#  Section 1: Core Autodiff Engine (Node Class - Unchanged)
# ==============================================================================
# The Node class is the foundation and remains the same.
def _sum_to_shape(grad, shape):
    while grad.ndim > len(shape): grad = grad.sum(axis=0)
    for i, (g_dim, t_dim) in enumerate(zip(grad.shape, shape)):
        if t_dim == 1 and g_dim != 1: grad = grad.sum(axis=i, keepdims=True)
    return grad.reshape(shape)

class Node:
    def __init__(self, value, parents=(), op=''):
        self.value = np.array(value, dtype=float); self.parents = parents; self.op = op
        self.grad = None; self._backward = lambda: None
    def _ensure(self, other):
        return other if isinstance(other, Node) else Node(np.array(other, dtype=float))
    def __add__(self, other):
        other = self._ensure(other); out = Node(self.value + other.value, (self, other), '+')
        def _backward():
            if out.grad is None: return
            self.grad += _sum_to_shape(out.grad, self.value.shape)
            other.grad += _sum_to_shape(out.grad, other.value.shape)
        out._backward = _backward; return out
    def __mul__(self, other):
        other = self._ensure(other); out = Node(self.value * other.value, (self, other), '*')
        def _backward():
            if out.grad is None: return
            self.grad += _sum_to_shape(out.grad * other.value, self.value.shape)
            other.grad += _sum_to_shape(out.grad * self.value, other.value.shape)
        out._backward = _backward; return out
    def __pow__(self, power):
        assert isinstance(power, (int, float)); out = Node(self.value ** power, (self,), f'**{power}')
        def _backward():
            if out.grad is None: return
            self.grad += (power * self.value**(power-1)) * out.grad
        out._backward = _backward; return out
    def __matmul__(self, other):
        other = self._ensure(other); out = Node(self.value @ other.value, (self, other), '@')
        def _backward():
            if out.grad is None: return
            A, B, G = self.value, other.value, out.grad
            if A.ndim == 2 and B.ndim == 1: self_grad, other_grad = np.outer(G, B), A.T @ G
            else: self_grad, other_grad = G @ B.T, A.T @ G
            self.grad += _sum_to_shape(self_grad, self.value.shape)
            other.grad += _sum_to_shape(other_grad, other.value.shape)
        out._backward = _backward; return out
    def relu(self):
        out = Node(np.maximum(0, self.value), (self,), 'relu')
        def _backward():
            if out.grad is None: return
            self.grad += (self.value > 0) * out.grad
        out._backward = _backward; return out
    def tanh(self):
        out = Node(np.tanh(self.value), (self,), 'tanh')
        def _backward():
            if out.grad is None: return
            self.grad += (1 - out.value**2) * out.grad
        out._backward = _backward; return out
    def sum(self):
        out = Node(self.value.sum(), (self,), 'sum')
        def _backward():
            if out.grad is None: return
            self.grad += np.broadcast_to(out.grad, self.value.shape)
        out._backward = _backward; return out
    def backward(self):
        topo, visited = [], set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for p in v.parents: build_topo(p)
                topo.append(v)
        build_topo(self)
        for v in topo: v.grad = np.zeros_like(v.value)
        self.grad = np.ones_like(self.value)
        for v in reversed(topo): v._backward()

# ==============================================================================
#  Section 2: The New 'nn' and 'optim' Packages
# ==============================================================================

class nn:
    class Module:
        """Base class for all neural network modules."""
        def __init__(self):
            self._modules = OrderedDict()
            self._params = OrderedDict()

        def __setattr__(self, name, value):
            if isinstance(value, nn.Module):
                self._modules[name] = value
            elif isinstance(value, Node):
                self._params[name] = value
            super().__setattr__(name, value)

        def parameters(self):
            """Return an iterator over module parameters."""
            for name, param in self._params.items():
                yield param
            for name, module in self._modules.items():
                yield from module.parameters()
        
        def zero_grad(self):
            """Sets gradients of all parameters to zero."""
            for p in self.parameters():
                p.grad = np.zeros_like(p.value)

        def __call__(self, *args, **kwargs):
            return self.forward(*args, **kwargs)

    class Linear(Module):
        """A linear transformation layer: y = xW + b."""
        def __init__(self, in_features, out_features):
            super().__init__()
            self.weight = Node(np.random.randn(in_features, out_features) * 0.1)
            self.bias = Node(np.zeros(out_features))
        
        def forward(self, x):
            return x @ self.weight + self.bias

    class Tanh(Module):
        """Tanh activation function."""
        def forward(self, x):
            return x.tanh()

    class ReLU(Module):
        """ReLU activation function."""
        def forward(self, x):
            return x.relu()

    class Sequential(Module):
        """A sequential container for modules."""
        def __init__(self, *args):
            super().__init__()
            for i, module in enumerate(args):
                self._modules[str(i)] = module
        
        def forward(self, x):
            for module in self._modules.values():
                x = module(x)
            return x

class optim:
    class Adam:
        """Adam optimizer."""
        def __init__(self, params, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8):
            self.params = list(params)
            self.lr = lr
            self.beta1 = beta1
            self.beta2 = beta2
            self.eps = eps
            self.t = 0
            self.m = [np.zeros_like(p.value) for p in self.params]
            self.v = [np.zeros_like(p.value) for p in self.params]

        def step(self):
            """Performs a single optimization step."""
            self.t += 1
            for i, p in enumerate(self.params):
                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * p.grad
                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (p.grad**2)
                m_hat = self.m[i] / (1 - self.beta1**self.t)
                v_hat = self.v[i] / (1 - self.beta2**self.t)
                p.value -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)
        
        def zero_grad(self):
            """A convenience method to call zero_grad on the model."""
            for p in self.params:
                p.grad = np.zeros_like(p.value)

# ==============================================================================
#  Section 3: Demonstration of the New Framework
# ==============================================================================

if __name__ == "__main__":
    print("--- Demonstrating the new, formalized framework ---")
    
    # 1. Define a model using our new, clean API
    np.random.seed(42)
    model = nn.Sequential(
        nn.Linear(4, 32),
        nn.Tanh(),
        nn.Linear(32, 16),
        nn.ReLU(),
        nn.Linear(16, 1)
    )
    
    # 2. Create an optimizer
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    
    # 3. Create some data
    X_train = Node(np.random.randn(100, 4))
    y_train = Node(np.sin(X_train.value[:, 0]) + 0.1)
    
    # 4. The new, elegant training loop
    start_time = time.time()
    print("Starting training...")
    for epoch in range(101):
        # Forward pass
        y_pred = model(X_train)
        
        # Compute loss (MSE)
        # Note: A proper nn.MSELoss() class would be the next step
        loss = (((y_pred + (y_train * -1)) * (y_pred + (y_train * -1)))).sum() * (1/100)
        
        # Backward pass and optimization
        optimizer.zero_grad() # Clear old gradients
        loss.backward()
        optimizer.step()      # Update weights
        
        if epoch % 20 == 0:
            print(f"Epoch {epoch:3d} | Loss: {loss.value.item():.6f}")
            
    end_time = time.time()
    
    print("\n--- Training Complete ---")
    print(f"Total training time: {end_time - start_time:.2f} seconds.")
    print("Final Loss:", loss.value.item())
    
    print("\nThis demonstrates a much cleaner, more modular, and reusable framework.")
    print("The training loop now closely resembles professional libraries like PyTorch.")

