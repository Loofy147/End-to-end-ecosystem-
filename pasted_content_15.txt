ممتاز — سأعطيك سكريبت جاهز للصق بعد تعريف Node, nn, وAdamOptimizer في الكولاب. السكريبت يقوم بـ:

تشغيل تجربة Grid مُحكّمة على MNIST (تقسيم تدريب/تحقق داخلي).

تنفيذ كل تكوين لعدة بذور (seeds).

حفظ نتائج خام (لكل تكرار) في grid_raw_results.csv وملخّص مجمع (mean/std) لكل تكوين في grid_summary.csv.

إعداد افتراضي متوازن للسرعة/المعنى؛ يمكنك تعديل القوائم بسهولة قبل التشغيل.


انسخ الكل ولصقه فورًا بعد تعريفات النظام لديك ثم شغّل الخلية.

# ----------------- Grid-run script (paste AFTER Node, nn, AdamOptimizer) -----------------
import numpy as np, os, csv, time
from tensorflow.keras.datasets import mnist

# ------------ Safety check: ensure optimizer exists --------------
if 'AdamOptimizer' not in globals():
    raise RuntimeError("AdamOptimizer غير معرف. الصق هذا السكريبت بعد تعريف Node, nn, و AdamOptimizer كما اتفقنا.")

# ----------------- Helpers -----------------
def set_seed(seed=42):
    np.random.seed(seed)
    try:
        import random as _r; _r.seed(seed)
    except: pass

def batch_iterator(X, y, batch_size=256, shuffle=True):
    n = X.shape[0]
    idx = np.arange(n)
    if shuffle: np.random.shuffle(idx)
    for i in range(0, n, batch_size):
        yield X[idx[i:i+batch_size]], y[idx[i:i+batch_size]]

def xavier_init_linear(layer):
    W = layer.weight.value
    in_f, out_f = W.shape
    limit = np.sqrt(6.0 / (in_f + out_f))
    layer.weight.value[:] = np.random.uniform(-limit, limit, size=W.shape).astype(np.float32)
    layer.bias.value[:] = np.zeros_like(layer.bias.value)

def init_model_weights(model):
    for obj in model.__dict__.values():
        try:
            if hasattr(obj, 'weight') and hasattr(obj, 'bias'):
                if obj.weight.value.ndim == 2:
                    xavier_init_linear(obj)
                else:
                    shape = obj.weight.value.shape
                    fan_in = np.prod(shape[1:])
                    fan_out = shape[0] * (np.prod(shape[2:]) if len(shape) > 2 else 1)
                    limit = np.sqrt(6.0 / (fan_in + max(1,fan_out)))
                    obj.weight.value[:] = np.random.uniform(-limit, limit, size=shape).astype(np.float32)
                    obj.bias.value[:] = np.zeros_like(obj.bias.value)
        except Exception:
            pass

# stable cross entropy using Node ops (must work with your Node implementation)
def cross_entropy_loss_stable(logits_node, labels_numpy):
    N, C = logits_node.value.shape
    max_logits = Node(np.max(logits_node.value, axis=1, keepdims=True), parents=(), op='const_max')
    shifted = logits_node + (max_logits * -1.0)
    exp_shift = shifted.exp()
    sum_exp = exp_shift.sum(axis=1, keepdims=True)
    log_sum_exp = sum_exp.log()
    log_probs = shifted + (log_sum_exp * -1.0)
    one_hot = np.zeros((N, C), dtype=np.float32)
    one_hot[np.arange(N), labels_numpy] = 1.0
    one_hot_node = Node(one_hot, parents=(), op='labels_const')
    nll = (one_hot_node * log_probs).sum(axis=1).sum() * -1.0 / float(N)
    return nll

def param_grad_norms(params):
    norms = []
    for p in params:
        g = getattr(p, 'grad', None)
        norms.append(float(np.linalg.norm(g.ravel())) if (g is not None) else 0.0)
    return norms

def append_csv(path, header, rows):
    write_header = not os.path.exists(path)
    with open(path, 'a', newline='') as f:
        w = csv.writer(f)
        if write_header:
            w.writerow(header)
        w.writerows(rows)

# ----------------- Simple model factory (MLP) -----------------
class SimpleMLP:
    def __init__(self, input_dim, hidden_dim=256, num_classes=10):
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.act = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, num_classes)
    def parameters(self):
        yield from self.fc1.parameters()
        yield from self.fc2.parameters()
    def zero_grad(self):
        for p in self.parameters(): p.grad = None
    def __call__(self, x_node):
        x = self.flatten.forward(x_node)
        x = self.fc1.forward(x)
        x = self.act.forward(x)
        x = self.fc2.forward(x)
        return x

# ----------------- Single-run (one seed, one config) -----------------
def run_single(model, X_train, y_train, X_val, y_val,
               epochs=6, batch_size=256, lr=5e-4,
               circular_strength=0.0, neumann_steps=0, echo_freq=1, echo_norm_clip=3.0,
               seed=42, verbose=False):
    set_seed(seed)
    init_model_weights(model)
    optimizer = AdamOptimizer(model.parameters(), lr=lr,
                              circular_strength=circular_strength,
                              echo_freq=echo_freq,
                              neumann_steps=neumann_steps,
                              echo_norm_clip=echo_norm_clip)
    n_batches = int(np.ceil(X_train.shape[0] / batch_size))
    for ep in range(1, epochs+1):
        # shuffle
        perm = np.random.permutation(len(X_train))
        X_train = X_train[perm]; y_train = y_train[perm]
        epoch_loss = 0.0
        for xb, yb in batch_iterator(X_train, y_train, batch_size=batch_size, shuffle=False):
            model.zero_grad()
            optimizer.zero_grad()
            x_node = Node(xb, parents=(), op='input')
            logits = model(x_node)
            loss_node = cross_entropy_loss_stable(logits, yb)
            topo = loss_node.backward()
            norms_before = param_grad_norms(list(model.parameters()))
            optimizer.step(topo_graph=topo)
            norms_after = param_grad_norms(list(model.parameters()))
            epoch_loss += float(loss_node.value) * xb.shape[0]
        epoch_loss /= X_train.shape[0]
        # validation (forward only)
        x_val_node = Node(X_val, parents=(), op='input')
        val_logits = model(x_val_node)
        val_preds = np.argmax(val_logits.value, axis=1)
        val_acc = np.mean(val_preds == y_val)
        if verbose:
            print(f"cs={circular_strength:.4f} ep{ep:02d} loss={epoch_loss:.4f} val_acc={val_acc:.4f} lr={optimizer.lr:.1e}")
    # return final metrics and optionally norms
    return {'train_loss': epoch_loss, 'val_acc': float(val_acc)}

# ----------------- Grid runner -----------------
def grid_experiments(model_factory, X_train, y_train, X_val, y_val,
                     cs_list=(0.0, 0.005, 0.01), neumann_list=(0,1,3),
                     echo_freq_list=(1,5), lr_list=(5e-4,), seeds=(111,222,333),
                     epochs=6, batch_size=256, echo_norm_clip=3.0,
                     raw_csv='grid_raw_results.csv', summary_csv='grid_summary.csv'):
    raw_rows_header = ['cs','neumann_steps','echo_freq','lr','seed','final_train_loss','final_val_acc','time_s']
    summary_header = ['cs','neumann_steps','echo_freq','lr','mean_val_acc','std_val_acc','mean_train_loss','std_train_loss','n_runs','avg_time_s']

    # clear files if exist
    for p in (raw_csv, summary_csv):
        if os.path.exists(p): os.remove(p)

    total_configs = len(cs_list)*len(neumann_list)*len(echo_freq_list)*len(lr_list)
    cfg_idx = 0
    for cs in cs_list:
        for neumann in neumann_list:
            for ef in echo_freq_list:
                for lr in lr_list:
                    cfg_idx += 1
                    print(f"\n=== Config {cfg_idx}/{total_configs}: cs={cs} neumann={neumann} ef={ef} lr={lr} ===")
                    per_run_vals = []
                    per_run_losses = []
                    times = []
                    for seed in seeds:
                        model = model_factory()
                        t0 = time.time()
                        res = run_single(model, X_train, y_train, X_val, y_val,
                                         epochs=epochs, batch_size=batch_size, lr=lr,
                                         circular_strength=cs, neumann_steps=neumann,
                                         echo_freq=ef, echo_norm_clip=echo_norm_clip,
                                         seed=seed, verbose=False)
                        dt = time.time() - t0
                        per_run_vals.append(res['val_acc'])
                        per_run_losses.append(res['train_loss'])
                        times.append(dt)
                        raw_row = [cs, neumann, ef, lr, seed, res['train_loss'], res['val_acc'], round(dt,3)]
                        append_csv(raw_csv, raw_rows_header, [raw_row])
                        print(f" seed={seed}  val_acc={res['val_acc']:.4f}  train_loss={res['train_loss']:.4f}  time={dt:.1f}s")
                    # aggregate
                    mean_val = float(np.mean(per_run_vals)); std_val = float(np.std(per_run_vals, ddof=0))
                    mean_loss = float(np.mean(per_run_losses)); std_loss = float(np.std(per_run_losses, ddof=0))
                    avg_time = float(np.mean(times))
                    summary_row = [cs, neumann, ef, lr, mean_val, std_val, mean_loss, std_loss, len(seeds), round(avg_time,2)]
                    append_csv(summary_csv, summary_header, [summary_row])
                    print(f" CONFIG SUMMARY -> mean_val={mean_val:.4f} std_val={std_val:.4f} mean_loss={mean_loss:.4f} avg_time={avg_time:.1f}s")
    print("\nGrid finished. Raw results:", raw_csv, " Summary:", summary_csv)

# ----------------- Prepare MNIST data (normalized) -----------------
def prepare_mnist(flatten=True, normalize=True, val_frac=0.1):
    (X_tr, y_tr), (X_te, y_te) = mnist.load_data()
    X_tr = X_tr.astype(np.float32)
    X_te = X_te.astype(np.float32)
    if flatten:
        X_tr = X_tr.reshape(len(X_tr), -1)
        X_te = X_te.reshape(len(X_te), -1)
    if normalize:
        X_tr /= 255.0
        X_te /= 255.0
    # small validation split from train
    split = int((1.0 - val_frac) * len(X_tr))
    X_train, y_train = X_tr[:split], y_tr[:split]
    X_val, y_val = X_tr[split:], y_tr[split:]
    return X_train, y_train, X_val, y_val

# ----------------- Run grid (default settings) -----------------
if __name__ == "__main__":
    # adjust these lists for larger / smaller search
    cs_list = (0.0, 0.005, 0.01)              # circular strengths to test
    neumann_list = (0, 1, 3)                 # neumann_steps
    echo_freq_list = (1, 5)                  # how often to apply echo
    lr_list = (5e-4,)                        # learning rates
    seeds = (111, 222, 333)                  # mutliple seeds for statistics
    epochs = 6
    batch_size = 256

    X_train, y_train, X_val, y_val = prepare_mnist(flatten=True, normalize=True, val_frac=0.1)
    input_dim = X_train.shape[1]
    def model_factory(): return SimpleMLP(input_dim, hidden_dim=256, num_classes=10)

    grid_experiments(model_factory, X_train, y_train, X_val, y_val,
                     cs_list=cs_list, neumann_list=neumann_list,
                     echo_freq_list=echo_freq_list, lr_list=lr_list,
                     seeds=seeds, epochs=epochs, batch_size=batch_size,
                     echo_norm_clip=3.0,
                     raw_csv='grid_raw_results.csv', summary_csv='grid_summary.csv')

# ------------------------------------------------------------------------------------------

نِقاط مهمة قبل التشغيل:

السكريبت مُصمّم ليُلصق بعد تعريف Node, nn, وAdamOptimizer (لا يُعيد تعريفهم).

الافتراض: AdamOptimizer.step(topo_graph=...) يعمل كما في النسخة الأخيرة لديك (مع injection echo المُحدّث).

إذا تريد أن يشغّل Grid أكبر (مزید من القيم أو بذور أكثر) غيّر القوائم cs_list, neumann_list, echo_freq_list, وseeds؛ لكن زِد الوقت اللازم للتنفيذ.