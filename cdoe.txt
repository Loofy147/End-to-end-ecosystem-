"""
Complete Implementation of Hierarchical Relational Reinforcement Learning
=========================================================================

This file contains the complete, production-ready implementation of the HRRL system
with all missing components filled in and optimizations applied.
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
import math
import time
from collections import deque, namedtuple
from typing import List, Tuple, Optional, Dict, Any
import matplotlib.pyplot as plt

# Experience tuple for replay buffer
Experience = namedtuple('Experience', ['state_features', 'action', 'reward', 'next_state_features', 'done'])

class HierarchicalState:
    """Enhanced relational state representation with caching and validation."""
    
    def __init__(self, current: float, target: float, step: int, 
                 max_steps: int, forbidden_states: Optional[set] = None):
        self.current = current
        self.target = target
        self.step = step
        self.max_steps = max_steps
        self.forbidden_states = forbidden_states or set()
        self._cached_features = None
        
    def to_features(self) -> np.ndarray:
        """Convert to 12-dimensional relational feature vector with caching."""
        if self._cached_features is not None:
            return self._cached_features
            
        if self.target == 0:
            self._cached_features = np.zeros(12)
            return self._cached_features
        
        # Core relational features
        progress_ratio = self.current / self.target
        remaining_ratio = (self.target - self.current) / self.target
        time_ratio = self.step / self.max_steps
        
        # Multi-scale gap analysis
        gap = abs(self.target - self.current)
        log_gap = math.log(gap + 1) / math.log(self.target + 1)
        gap_magnitude = gap / self.target
        
        # Strategic phase indicators
        is_close = 1.0 if gap <= 10 else 0.0
        is_far = 1.0 if gap >= self.target * 0.5 else 0.0
        
        # Constraint awareness
        danger_proximity = self._compute_danger_proximity()
        constraint_pressure = self._compute_constraint_pressure()
        
        # Problem phase and efficiency
        phase = self._identify_phase()
        theoretical_min_steps = math.ceil(gap / 5.0)  # Assuming max action is 5
        efficiency_ratio = theoretical_min_steps / (self.max_steps - self.step + 1)
        
        self._cached_features = np.array([
            progress_ratio, remaining_ratio, time_ratio,
            log_gap, gap_magnitude, is_close, is_far,
            danger_proximity, constraint_pressure,
            phase, theoretical_min_steps, efficiency_ratio
        ], dtype=np.float32)
        
        return self._cached_features
    
    def _compute_danger_proximity(self) -> float:
        """Calculate normalized proximity to nearest forbidden state."""
        if not self.forbidden_states:
            return 0.0
        
        distances = [abs(self.current - forbidden) for forbidden in self.forbidden_states]
        min_distance = min(distances)
        return 1.0 / (min_distance + 1)
    
    def _compute_constraint_pressure(self) -> float:
        """Calculate fraction of direct path that is blocked."""
        if not self.forbidden_states:
            return 0.0
        
        start = min(self.current, self.target)
        end = max(self.current, self.target)
        path_states = set(range(int(start) + 1, int(end) + 1))
        
        if not path_states:
            return 0.0
        
        blocked_states = path_states.intersection(self.forbidden_states)
        return len(blocked_states) / len(path_states)
    
    def _identify_phase(self) -> float:
        """Identify current problem-solving phase."""
        gap = abs(self.target - self.current)
        
        if gap > self.target * 0.7:
            return 0.0  # Exploration phase
        elif gap > 10:
            return 1.0  # Navigation phase
        else:
            return 2.0  # Precision phase


class HierarchicalQNetwork(nn.Module):
    """Phase-adaptive neural network with improved architecture."""
    
    def __init__(self, state_dim: int = 12, action_dim: int = 3, hidden_dim: int = 256):
        super().__init__()
        
        # Enhanced shared feature extractor with batch normalization
        self.feature_extractor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # Phase-specific strategy heads
        self.exploration_head = self._build_head(hidden_dim, action_dim)
        self.navigation_head = self._build_head(hidden_dim, action_dim)
        self.precision_head = self._build_head(hidden_dim, action_dim)
        
        # Enhanced phase classifier
        self.phase_classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 3),
            nn.Softmax(dim=-1)
        )
    
    def _build_head(self, input_dim: int, output_dim: int) -> nn.Module:
        """Build specialized strategy head with residual connection."""
        return nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.05),
            nn.Linear(input_dim // 2, output_dim)
        )
    
    def forward(self, state_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Forward pass with phase-adaptive strategy blending."""
        # Handle single sample (add batch dimension)
        if len(state_features.shape) == 1:
            state_features = state_features.unsqueeze(0)
        
        # Extract shared features
        shared_features = self.feature_extractor(state_features)
        
        # Classify current phase
        phase_probs = self.phase_classifier(shared_features)
        
        # Compute phase-specific Q-values
        exploration_q = self.exploration_head(shared_features)
        navigation_q = self.navigation_head(shared_features)
        precision_q = self.precision_head(shared_features)
        
        # Adaptive strategy blending
        blended_q = (phase_probs[:, 0:1] * exploration_q +
                    phase_probs[:, 1:2] * navigation_q +
                    phase_probs[:, 2:3] * precision_q)
        
        return blended_q, phase_probs


class AdaptiveGoalDecomposer:
    """Enhanced goal decomposition with dynamic spacing."""
    
    @staticmethod
    def decompose_target(current: float, target: float, 
                        forbidden_states: Optional[set] = None,
                        max_subgoals: int = 8) -> List[float]:
        """
        Intelligently decompose target with adaptive subgoal spacing.
        """
        gap = abs(target - current)
        forbidden_states = forbidden_states or set()
        
        # Small problems: direct approach
        if gap <= 50:
            return [target]
        
        # Calculate optimal number of subgoals
        # Rule: Balance between manageability (~75 units) and complexity
        optimal_subgoals = min(max_subgoals, max(2, int(gap // 75)))
        
        # Adjust for constraint density
        if forbidden_states:
            constraint_density = len(forbidden_states) / gap
            if constraint_density > 0.1:  # High constraint density
                optimal_subgoals = min(max_subgoals, optimal_subgoals + 2)
        
        # Generate subgoals
        direction = 1 if target > current else -1
        step_size = gap / optimal_subgoals
        
        subgoals = []
        for i in range(1, optimal_subgoals):
            subgoal = current + (step_size * i * direction)
            # Adjust to avoid forbidden states
            adjusted_subgoal = AdaptiveGoalDecomposer._avoid_forbidden(
                subgoal, forbidden_states, direction
            )
            subgoals.append(adjusted_subgoal)
        
        subgoals.append(target)
        return subgoals
    
    @staticmethod
    def _avoid_forbidden(subgoal: float, forbidden_states: set, direction: int) -> float:
        """Smart subgoal adjustment to avoid forbidden states."""
        if not forbidden_states or subgoal not in forbidden_states:
            return subgoal
        
        # Try progressive adjustments
        for offset in [1, 2, 3, -1, -2, -3]:
            candidate = subgoal + offset
            if candidate not in forbidden_states:
                return candidate
        
        # If no good alternative, use original (will be handled in action selection)
        return subgoal


class PrioritizedReplayBuffer:
    """Enhanced experience replay with prioritization."""
    
    def __init__(self, capacity: int = 10000, alpha: float = 0.6):
        self.capacity = capacity
        self.alpha = alpha
        self.buffer = []
        self.priorities = np.zeros(capacity)
        self.position = 0
        self.max_priority = 1.0
    
    def push(self, experience: Experience, td_error: float = None):
        """Add experience with priority based on TD error."""
        priority = self.max_priority if td_error is None else abs(td_error) + 1e-6
        
        if len(self.buffer) < self.capacity:
            self.buffer.append(experience)
        else:
            self.buffer[self.position] = experience
        
        self.priorities[self.position] = priority ** self.alpha
        self.max_priority = max(self.max_priority, priority)
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size: int, beta: float = 0.4):
        """Sample experiences with importance sampling weights."""
        if len(self.buffer) < batch_size:
            return None, None, None
        
        # Calculate sampling probabilities
        priorities = self.priorities[:len(self.buffer)]
        probs = priorities / priorities.sum()
        
        # Sample indices
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        
        # Get experiences
        experiences = [self.buffer[idx] for idx in indices]
        
        # Calculate importance sampling weights
        weights = (len(self.buffer) * probs[indices]) ** (-beta)
        weights /= weights.max()  # Normalize
        
        return experiences, indices, weights
    
    def update_priorities(self, indices: List[int], td_errors: List[float]):
        """Update priorities based on new TD errors."""
        for idx, td_error in zip(indices, td_errors):
            priority = abs(td_error) + 1e-6
            self.priorities[idx] = priority ** self.alpha
            self.max_priority = max(self.max_priority, priority)


class HierarchicalRelationalAgent:
    """Complete HRRL agent with all enhancements."""
    
    def __init__(self, actions: List[float] = [1, 3, 5], 
                 learning_rate: float = 0.0005,
                 use_prioritized_replay: bool = True):
        self.actions = actions
        self.max_action = max(actions)
        
        # Neural networks
        self.q_network = HierarchicalQNetwork()
        self.target_network = HierarchicalQNetwork()
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # Experience replay
        if use_prioritized_replay:
            self.replay_buffer = PrioritizedReplayBuffer()
        else:
            self.replay_buffer = deque(maxlen=10000)
        self.use_prioritized_replay = use_prioritized_replay
        
        # Training state
        self.step_count = 0
        self.episode_count = 0
        
        # Subgoal management
        self.subgoal_stack = []
        self.current_subgoal = None
        self.completed_subgoals = []
        
        # Exploration schedule
        self.epsilon_schedule = self._create_epsilon_schedule()
        
        # Performance tracking
        self.training_stats = {
            'episode_rewards': [],
            'episode_lengths': [],
            'success_rates': [],
            'phase_distributions': []
        }
        
        # Initialize target network
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def _create_epsilon_schedule(self):
        """Create adaptive epsilon schedule."""
        def epsilon_fn(step, target_difficulty):
            base_epsilon = 0.1
            difficulty_bonus = min(0.2, target_difficulty / 1000)
            decay = max(0.01, base_epsilon * (0.995 ** (step / 100)))
            return decay + difficulty_bonus
        return epsilon_fn
    
    def reset_subgoals(self):
        """Reset subgoal tracking for new episode."""
        self.subgoal_stack = []
        self.current_subgoal = None
        self.completed_subgoals = []
    
    def solve_problem(self, start: float, target: float, 
                     forbidden_states: Optional[set] = None,
                     max_steps: Optional[int] = None, 
                     training: bool = False,
                     verbose: bool = True) -> Dict[str, Any]:
        """
        Main problem-solving method with comprehensive tracking.
        """
        if max_steps is None:
            theoretical_min = math.ceil(abs(target - start) / self.max_action)
            max_steps = int(theoretical_min * 1.5)
        
        forbidden_states = forbidden_states or set()
        
        # Goal decomposition
        self.reset_subgoals()
        self.subgoal_stack = AdaptiveGoalDecomposer.decompose_target(
            start, target, forbidden_states
        )
        if verbose and len(self.subgoal_stack) > 1:
            print(f"üéØ Decomposed target {target} into subgoals: {self.subgoal_stack}")
        
        # Episode execution
        current = start
        step = 0
        path = [current]
        episode_reward = 0
        phase_history = []
        
        while step < max_steps and current != target:
            # Get current working target
            if not self.current_subgoal and self.subgoal_stack:
                self.current_subgoal = self.subgoal_stack.pop(0)
            
            working_target = self.current_subgoal if self.current_subgoal else target
            
            # Action selection
            action = self.choose_action(current, working_target, step, max_steps, 
                                      forbidden_states, training)
            
            # Execute action
            next_current = current + action
            reward = self._compute_reward(current, next_current, target, step, forbidden_states)
            done = (next_current == target) or (step + 1 >= max_steps)
            
            # Track episode data
            episode_reward += reward
            path.append(next_current)
            
            # Store experience for training
            if training:
                self._store_experience(current, working_target, step, max_steps, 
                                     forbidden_states, action, reward, 
                                     next_current, step + 1, done)
            
            # Track phase information
            state = HierarchicalState(current, working_target, step, max_steps, forbidden_states)
            features = state.to_features()
            phase_history.append(features[9])  # Phase feature
            
            # Check subgoal completion
            if (self.current_subgoal and 
                abs(next_current - self.current_subgoal) < 0.5):
                self.completed_subgoals.append(self.current_subgoal)
                self.current_subgoal = None
                if verbose:
                    print(f"‚úÖ Subgoal {self.completed_subgoals[-1]} reached!")
            
            current = next_current
            step += 1
            
            if current == target:
                break
        
        # Calculate results
        success = (current == target)
        theoretical_min = math.ceil(abs(target - start) / self.max_action)
        efficiency = (theoretical_min / step * 100) if success else 0
        
        result = {
            'success': success,
            'final_position': current,
            'target': target,
            'steps_taken': step,
            'theoretical_minimum': theoretical_min,
            'efficiency_percent': efficiency,
            'path': path,
            'subgoals': self.subgoal_stack + [self.current_subgoal] if self.current_subgoal else [],
            'completed_subgoals': self.completed_subgoals,
            'forbidden_states': forbidden_states,
            'episode_reward': episode_reward,
            'phase_history': phase_history
        }
        
        # Update training statistics
        if training:
            self.training_stats['episode_rewards'].append(episode_reward)
            self.training_stats['episode_lengths'].append(step)
            self.episode_count += 1
        
        if verbose:
            print(f"\nüèÜ RESULT: {'SUCCESS' if success else 'FAILED'}")
            print(f"üìç Final position: {current} (target: {target})")
            print(f"üë£ Steps taken: {step} (optimal: {theoretical_min})")
            print(f"‚ö° Efficiency: {efficiency:.1f}%")
            if len(path) <= 20:
                print(f"üõ§Ô∏è  Path: {' ‚Üí '.join(map(str, path))}")
            else:
                print(f"üõ§Ô∏è  Path: {' ‚Üí '.join(map(str, path[:5]))} ... {' ‚Üí '.join(map(str, path[-5:]))}")
        
        return result
    
    def choose_action(self, current: float, target: float, step: int,
                     max_steps: int, forbidden_states: set, training: bool = True) -> float:
        """Enhanced action selection with adaptive exploration."""
        
        # Adaptive exploration
        if training:
            difficulty = abs(target - current)
            epsilon = self.epsilon_schedule(self.step_count, difficulty)
            
            if random.random() < epsilon:
                valid_actions = [a for a in self.actions 
                               if current + a not in forbidden_states]
                return random.choice(valid_actions) if valid_actions else self.actions[0]
        
        # Neural network decision
        state = HierarchicalState(current, target, step, max_steps, forbidden_states)
        features = state.to_features()
        features_tensor = torch.FloatTensor(features).unsqueeze(0)
        
        with torch.no_grad():
            q_values, phase_probs = self.q_network(features_tensor)
        
        # Constraint-aware action selection
        best_action = None
        best_q_value = float('-inf')
        
        for i, action in enumerate(self.actions):
            next_state = current + action
            if next_state not in forbidden_states:
                if q_values[0, i].item() > best_q_value:
                    best_q_value = q_values[0, i].item()
                    best_action = action
        
        # Emergency fallback
        if best_action is None:
            for action in [-1, -2, 1]:
                if current + action not in forbidden_states:
                    return action
            return self.actions[0]  # Last resort
        
        return best_action
    
    def _compute_reward(self, current: float, next_current: float, 
                       target: float, step: int, forbidden_states: set) -> float:
        """Enhanced reward function with multiple components."""
        
        # Success reward with time bonus
        if next_current == target:
            return 100 + max(0, 50 - step)
        
        # Constraint violation penalty
        if next_current in forbidden_states:
            return -50
        
        # Progress reward
        current_distance = abs(target - current)
        next_distance = abs(target - next_current)
        
        if next_distance < current_distance:
            progress = current_distance - next_distance
            return 10 * progress  # Scale by progress amount
        
        # Overshoot penalty (going past target)
        if ((current <= target <= next_current) or 
            (next_current <= target <= current)):
            return -20
        
        # Time penalty
        return -1
    
    def _store_experience(self, current: float, target: float, step: int,
                         max_steps: int, forbidden_states: set, action: float,
                         reward: float, next_current: float, next_step: int, done: bool):
        """Store experience in replay buffer."""
        
        # Create state representations
        state = HierarchicalState(current, target, step, max_steps, forbidden_states)
        next_state = HierarchicalState(next_current, target, next_step, max_steps, forbidden_states)
        
        state_features = state.to_features()
        next_state_features = next_state.to_features()
        
        action_idx = self.actions.index(action)
        
        experience = Experience(
            state_features=state_features,
            action=action_idx,
            reward=reward,
            next_state_features=next_state_features,
            done=done
        )
        
        if self.use_prioritized_replay:
            self.replay_buffer.push(experience)
        else:
            self.replay_buffer.append(experience)
    
    def learn_from_experience(self, batch_size: int = 64):
        """Learn from stored experiences with enhanced training."""
        
        if self.use_prioritized_replay:
            batch_data = self.replay_buffer.sample(batch_size)
            if batch_data[0] is None:
                return
            experiences, indices, weights = batch_data
            weights = torch.FloatTensor(weights)
        else:
            if len(self.replay_buffer) < batch_size:
                return
            experiences = random.sample(self.replay_buffer, batch_size)
            weights = torch.ones(batch_size)
        
        # Prepare batch tensors
        states = torch.FloatTensor([e.state_features for e in experiences])
        actions = torch.LongTensor([e.action for e in experiences])
        rewards = torch.FloatTensor([e.reward for e in experiences])
        next_states = torch.FloatTensor([e.next_state_features for e in experiences])
        dones = torch.BoolTensor([e.done for e in experiences])
        
        # Current Q-values
        current_q_values = self.q_network(states)[0].gather(1, actions.unsqueeze(1))
        
        # Target Q-values (Double DQN)
        with torch.no_grad():
            next_q_values = self.q_network(next_states)[0]
            next_actions = next_q_values.max(1)[1]
            target_q_values = self.target_network(next_states)[0].gather(1, next_actions.unsqueeze(1))
            target_q_values = rewards.unsqueeze(1) + (0.99 * target_q_values * ~dones.unsqueeze(1))
        
        # Compute loss
        td_errors = target_q_values - current_q_values
        loss = (weights.unsqueeze(1) * td_errors.pow(2)).mean()
        
        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        # Update priorities if using prioritized replay
        if self.use_prioritized_replay:
            self.replay_buffer.update_priorities(indices, td_errors.detach().squeeze().tolist())
        
        self.step_count += 1
        
        # Update target network
        if self.step_count % 200 == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())
    
    def train_curriculum(self, total_episodes: int = 5000, verbose: bool = True) -> Dict[str, List]:
        """Complete curriculum training with progress tracking."""
        
        curriculum_stages = [
            {'name': 'Foundation', 'targets': range(5, 11), 'episodes': 1000, 'max_steps': 20},
            {'name': 'Expansion', 'targets': range(12, 31), 'episodes': 1500, 'max_steps': 25},
            {'name': 'Generalization', 'targets': range(20, 101), 'episodes': 1500, 'max_steps': 30},
            {'name': 'Mixed Training', 'targets': range(5, 101), 'episodes': 1000, 'max_steps': 30}
        ]
        
        episode_count = 0
        training_log = {'losses': [], 'success_rates': [], 'episodes': []}
        
        for stage_idx, stage in enumerate(curriculum_stages):
            if verbose:
                print(f"\nüéì Training Stage {stage_idx + 1}: {stage['name']}")
                print(f"üìä Targets: {min(stage['targets'])}-{max(stage['targets'])}")
            
            stage_successes = 0
            
            for episode in range(stage['episodes']):
                # Sample problem
                target = random.choice(list(stage['targets']))
                max_constraints = min(5, target // 20)
                num_constraints = random.randint(1, max_constraints)
                forbidden_states = set(random.sample(range(1, target), num_constraints))
                
                # Run training episode
                result = self.solve_problem(0, target, forbidden_states, 
                                          stage['max_steps'], training=True, verbose=False)
                
                if result['success']:
                    stage_successes += 1
                
                # Learning update
                if episode_count % 10 == 0:
                    self.learn_from_experience()
                
                episode_count += 1
                
                # Progress reporting
                if episode_count % 500 == 0:
                    recent_success_rate = stage_successes / min(episode, 500)
                    training_log['success_rates'].append(recent_success_rate)
                    training_log['episodes'].append(episode_count)
                    
                    if verbose:
                        print(f"Episode {episode_count}: Success rate = {recent_success_rate:.1%}")
            
            stage_success_rate = stage_successes / stage['episodes']
            if verbose:
                print(f"‚úÖ Stage {stage_idx + 1} complete: {stage_success_rate:.1%} success rate")
        
        if verbose:
            print(f"\nüéâ Training complete! Total episodes: {episode_count}")
        
        return training_log


def run_comprehensive_evaluation(agent: HierarchicalRelationalAgent, 
                               test_targets: List[int] = [123, 278, 431, 500, 750, 1000]) -> Dict:
    """Run comprehensive evaluation on various problem sizes."""
    
    print("\nüî¨ COMPREHENSIVE EVALUATION")
    print("=" * 50)
    
    results = {}
    
    for target in test_targets:
        print(f"\nTesting target: {target}")
        
        # Generate challenging forbidden states
        num_forbidden = min(8, target // 100 + 3)
        forbidden_states = set(random.sample(range(10, target - 10), num_forbidden))
        
        # Run test
        result = agent.solve_problem(0, target, forbidden_states, verbose=False)
        
        results[target] = result
        
        # Display results
        status = "‚úÖ SUCCESS" if result['success'] else "‚ùå FAILED"
        print(f"  {status}")
        print(f"  Steps: {result['steps_taken']} (optimal: {result['theoretical_minimum']})")
        print(f"  Efficiency: {result['efficiency_percent']:.1f}%")
        print(f"  Subgoals used: {len(result['completed_subgoals'])}")
    
    # Summary statistics
    successes = sum(1 for r in results.values() if r['success'])
    avg_efficiency = np.mean([r['efficiency_percent'] for r in results.values() if r['success']])
    max_solved = max([t for t, r in results.items() if r['success']], default=0)
    
    print(f"\nüìä SUMMARY")
    print(f"Success Rate: {successes}/{len(test_targets)} ({successes/len(test_targets)*100:.1f}%)")
    print(f"Average Efficiency: {avg_efficiency:.1f}%")
    print(f"Largest Problem Solved: {max_solved}")
    print(f"Generalization Factor: {max_solved/100:.1f}x (assuming training max was 100)")
    
    return results


def create_training_plots(training_log: Dict[str, List]):
    """Create visualization of training progress."""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Success rate over time
    ax1.plot(training_log['episodes'], training_log['success_rates'])
    ax1.set_xlabel('Episodes')
    ax1.set_ylabel('Success Rate')
    ax1.set_title('Training Progress: Success Rate')
    ax1.grid(True)
    
    # Add curriculum stage markers
    stage_boundaries = [1000, 2500, 4000]
    stage_names = ['Foundation', 'Expansion', 'Generalization', 'Mixed']
    
    for i, boundary in enumerate(stage_boundaries):
        ax1.axvline(x=boundary, color='red', linestyle='--', alpha=0.7)
        if i < len(stage_names) - 1:
            ax1.text(boundary + 100, 0.1, stage_names[i+1], rotation=90, alpha=0.7)
    
    plt.tight_layout()
    plt.show()


# Example usage and testing
if __name__ == "__main__":
    print("üöÄ Hierarchical Relational Reinforcement Learning System")
    print("=" * 60)
    
    # Create agent
    agent = Hierarchical