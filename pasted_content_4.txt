# Train/validation re-check for top configs from the 500-trial log.
import numpy as np, pandas as pd, json, time
from caas_jupyter_tools import display_dataframe_to_user

# Load CSV produced earlier
csv_path = '/mnt/data/orchestrator_500_results.csv'
df = pd.read_csv(csv_path)

# Select top 5 unique configs by mse
topk = df.sort_values('mse').head(20)  # pick more to ensure distinct
seen = set(); configs = []
for _, row in topk.iterrows():
    cfg_tuple = (int(row['H1']), int(row['H2']), float(row['lr']), int(row['seed']))
    if cfg_tuple not in seen:
        seen.add(cfg_tuple)
        configs.append({'H1':int(row['H1']), 'H2':int(row['H2']), 'lr':float(row['lr']), 'seed':int(row['seed']), 'epochs':50, 'activation1':'tanh'})
    if len(configs) >= 5:
        break

print("Top configs selected for train/val check:")
for i,c in enumerate(configs):
    print(i, c)

# Re-implement minimal Node and train function (same as used before)
def _sum_to_shape(grad, shape):
    while grad.ndim > len(shape):
        grad = grad.sum(axis=0)
    for i, (g_dim, t_dim) in enumerate(zip(grad.shape, shape)):
        if t_dim == 1 and g_dim != 1:
            grad = grad.sum(axis=i, keepdims=True)
    return grad.reshape(shape)

class Node:
    def __init__(self, value, parents=(), op=''):
        self.value = np.array(value, dtype=float)
        self.parents = parents
        self.op = op
        self.grad = None
        self._backward = lambda: None
    def _ensure(self, other):
        return other if isinstance(other, Node) else Node(np.array(other, dtype=float))
    def __add__(self, other):
        other = self._ensure(other); out = Node(self.value + other.value, (self, other), '+')
        def _backward():
            if out.grad is None: return
            self.grad = self.grad + _sum_to_shape(out.grad, self.value.shape) if self.grad is not None else _sum_to_shape(out.grad, self.value.shape)
            other.grad = other.grad + _sum_to_shape(out.grad, other.value.shape) if other.grad is not None else _sum_to_shape(out.grad, other.value.shape)
        out._backward = _backward; return out
    def __mul__(self, other):
        other = self._ensure(other); out = Node(self.value * other.value, (self, other), '*')
        def _backward():
            if out.grad is None: return
            self_grad = out.grad * other.value; other_grad = out.grad * self.value
            self.grad = self.grad + _sum_to_shape(self_grad, self.value.shape) if self.grad is not None else _sum_to_shape(self_grad, self.value.shape)
            other.grad = other.grad + _sum_to_shape(other_grad, other.value.shape) if other.grad is not None else _sum_to_shape(other_grad, other.value.shape)
        out._backward = _backward; return out
    def __sub__(self, other):
        other = self._ensure(other); out = Node(self.value - other.value, (self, other), '-')
        def _backward():
            if out.grad is None: return
            self.grad = self.grad + _sum_to_shape(out.grad, self.value.shape) if self.grad is not None else _sum_to_shape(out.grad, self.value.shape)
            other.grad = other.grad + _sum_to_shape(-out.grad, other.value.shape) if other.grad is not None else _sum_to_shape(-out.grad, other.value.shape)
        out._backward = _backward; return out
    def __truediv__(self, other):
        other = self._ensure(other); out = Node(self.value / other.value, (self, other), '/')
        def _backward():
            if out.grad is None: return
            self_grad = out.grad / other.value; other_grad = out.grad * (-self.value / (other.value ** 2))
            self.grad = self.grad + _sum_to_shape(self_grad, self.value.shape) if self.grad is not None else _sum_to_shape(self_grad, self.value.shape)
            other.grad = other.grad + _sum_to_shape(other_grad, other.value.shape) if other.grad is not None else _sum_to_shape(other_grad, other.value.shape)
        out._backward = _backward; return out
    def __matmul__(self, other):
        other = self._ensure(other); out = Node(self.value @ other.value, (self, other), '@')
        def _backward():
            if out.grad is None: return
            A=self.value; B=other.value; G=out.grad
            if A.ndim == 2 and B.ndim == 2:
                self_grad = G @ B.T; other_grad = A.T @ G
            elif A.ndim == 2 and B.ndim == 1:
                self_grad = (G[:, None] @ B[None, :]); other_grad = A.T @ G
            elif A.ndim == 1 and B.ndim == 2:
                self_grad = (G[None, :] @ B.T).reshape(A.shape); other_grad = np.outer(A, G)
            elif A.ndim == 1 and B.ndim == 1:
                self_grad = G * B; other_grad = G * A
            else:
                self_grad = np.matmul(G, B.T); other_grad = np.matmul(A.T, G)
            self.grad = self.grad + _sum_to_shape(self_grad, self.value.shape) if self.grad is not None else _sum_to_shape(self_grad, self.value.shape)
            other.grad = other.grad + _sum_to_shape(other_grad, other.value.shape) if other.grad is not None else _sum_to_shape(other_grad, other.value.shape)
        out._backward = _backward; return out
    def relu(self):
        out = Node(np.where(self.value > 0, self.value, 0.0), (self,), 'relu')
        def _backward():
            if out.grad is None: return
            self_grad = (self.value > 0).astype(float) * out.grad
            self.grad = self.grad + _sum_to_shape(self_grad, self.value.shape) if self.grad is not None else _sum_to_shape(self_grad, self.value.shape)
        out._backward = _backward; return out
    def tanh(self):
        out = Node(np.tanh(self.value), (self,), 'tanh')
        def _backward():
            if out.grad is None: return
            self_grad = (1 - np.tanh(self.value)**2) * out.grad
            self.grad = self.grad + _sum_to_shape(self_grad, self.value.shape) if self.grad is not None else _sum_to_shape(self_grad, self.value.shape)
        out._backward = _backward; return out
    def sum(self, axis=None, keepdims=False):
        out_val = self.value.sum(axis=axis, keepdims=keepdims); out = Node(out_val, (self,), 'sum')
        def _backward():
            if out.grad is None: return
            grad = out.grad
            if not keepdims and axis is None:
                grad = np.broadcast_to(out.grad, self.value.shape)
            elif not keepdims:
                shape = list(self.value.shape); axis_list = [axis] if isinstance(axis,int) else list(axis)
                for ax in sorted(axis_list):
                    grad = np.expand_dims(grad, ax)
                grad = np.broadcast_to(grad, self.value.shape)
            else:
                grad = np.broadcast_to(grad, self.value.shape)
            self.grad = self.grad + grad if self.grad is not None else grad
        out._backward = _backward; return out
    def backward(self, grad=None):
        topo=[]; visited=set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for p in v.parents:
                    build_topo(p)
                topo.append(v)
        build_topo(self)
        for v in topo:
            v.grad = np.zeros_like(v.value)
        if grad is None:
            grad = np.ones_like(self.value)
        self.grad = grad if self.grad is None else self.grad + grad
        for v in reversed(topo):
            v._backward()

# Training function with train/val split
def train_and_eval(config, val_frac=0.2, epochs=None):
    # generate dataset deterministic from seed
    seed = config.get('seed', 0)
    np.random.seed(seed)
    N = config.get('N', 120); D = config.get('D', 4)
    X_all = np.random.randn(N, D)
    y_all = np.sin(X_all[:,0]) + 0.5 * (X_all[:,1]**2) - 0.3 * X_all[:,2] + 0.2*np.tanh(X_all[:,3]) + 0.1 * np.random.randn(N)
    # split
    idx = np.arange(N)
    rng = np.random.RandomState(seed+1234)
    rng.shuffle(idx)
    n_val = int(N * val_frac)
    val_idx = idx[:n_val]; train_idx = idx[n_val:]
    X_train = X_all[train_idx]; y_train = y_all[train_idx]
    X_val = X_all[val_idx]; y_val = y_all[val_idx]
    # model init
    H1 = config.get('H1',24); H2 = config.get('H2',12)
    np.random.seed(config.get('seed',0))
    W1 = Node(np.random.randn(D, H1) * 0.1); b1 = Node(np.zeros(H1))
    W2 = Node(np.random.randn(H1, H2) * 0.1); b2 = Node(np.zeros(H2))
    W3 = Node(np.random.randn(H2) * 0.1); b3 = Node(np.array(0.0))
    params = [W1,b1,W2,b2,W3,b3]
    def init_adam(p): return {'m': np.zeros_like(p.value), 'v': np.zeros_like(p.value)}
    state = {k: init_adam(p) for k,p in zip(['W1','b1','W2','b2','W3','b3'], params)}
    beta1=0.9; beta2=0.999; eps=1e-8; lr=config.get('lr', 0.01)
    def adam_step(param, s, t):
        m_new = beta1 * s['m'] + (1-beta1) * param.grad
        v_new = beta2 * s['v'] + (1-beta2) * (param.grad * param.grad)
        s['m']=m_new; s['v']=v_new
        m_hat = s['m'] / (1 - beta1**t); v_hat = s['v'] / (1 - beta2**t)
        param.value = param.value - lr * m_hat / (np.sqrt(v_hat) + eps)
    E = epochs if epochs is not None else config.get('epochs', 50)
    loss_history = []
    for epoch in range(1, E+1):
        Xn = Node(X_train); yn = Node(y_train)
        h1 = Xn @ W1; h1 = h1 + b1
        h1 = h1.tanh() if config.get('activation1','tanh')=='tanh' else h1.relu()
        h2 = h1 @ W2; h2 = h2 + b2; h2 = h2.relu()
        pred = h2 @ W3; pred = pred + b3
        diff = pred - yn; loss = (diff*diff).sum() / X_train.shape[0]
        loss.backward()
        loss_history.append(float(loss.value))
        adam_step(W1, state['W1'], epoch); adam_step(b1, state['b1'], epoch)
        adam_step(W2, state['W2'], epoch); adam_step(b2, state['b2'], epoch)
        adam_step(W3, state['W3'], epoch); adam_step(b3, state['b3'], epoch)
    # evaluate train and val
    def forward_eval(X):
        h1 = np.tanh(X @ W1.value + b1.value)
        h2 = np.maximum(h1 @ W2.value + b2.value, 0.0)
        pred = h2 @ W3.value + b3.value
        return pred
    pred_train = forward_eval(X_train); pred_val = forward_eval(X_val)
    mse_train = float(np.mean((pred_train - y_train)**2)); mse_val = float(np.mean((pred_val - y_val)**2))
    r2_train = float(1 - mse_train/np.var(y_train)); r2_val = float(1 - mse_val/np.var(y_val))
    # compute final grad norm on train
    Xn = Node(X_train); yn = Node(y_train)
    h1 = Xn @ W1; h1 = h1 + b1; h1 = h1.tanh()
    h2 = h1 @ W2; h2 = h2 + b2; h2 = h2.relu()
    pred = h2 @ W3; pred = pred + b3; diff = pred - yn; loss = (diff*diff).sum() / X_train.shape[0]; loss.backward()
    grad_vec = np.concatenate([p.grad.ravel() for p in params])
    grad_norm = float(np.linalg.norm(grad_vec))
    return {'config': config, 'mse_train': mse_train, 'mse_val': mse_val, 'r2_train': r2_train, 'r2_val': r2_val, 'grad_norm': grad_norm, 'loss_history': loss_history}

# Run train/val for selected configs
results = []
start = time.time()
for cfg in configs:
    res = train_and_eval(cfg, val_frac=0.2, epochs=100)  # increase epochs for more robust training
    results.append(res)
end = time.time()
print("Train/val re-check completed in {:.2f}s".format(end-start))

# Present results
rows = []
for i, r in enumerate(results):
    cfg = r['config']
    rows.append({
        'rank': i+1,
        'H1': cfg['H1'],
        'H2': cfg['H2'],
        'lr': cfg['lr'],
        'seed': cfg['seed'],
        'mse_train': r['mse_train'],
        'mse_val': r['mse_val'],
        'r2_train': r['r2_train'],
        'r2_val': r['r2_val'],
        'grad_norm': r['grad_norm']
    })
df_res = pd.DataFrame(rows).sort_values('mse_val').reset_index(drop=True)
display_dataframe_to_user("Train/Val Check - Top Configs", df_res)
print(df_res.to_string(index=False))
results