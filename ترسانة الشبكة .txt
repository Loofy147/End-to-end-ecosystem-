import numpy as np
import time
import requests, gzip, os

# ==============================================================================
#  Section 0: Data Loading and Preparation
# ==============================================================================
def fetch_mnist():
    """Fetches and loads MNIST dataset."""
    def parse(filename):
        with gzip.open(filename, 'rb') as f:
            magic, num = np.frombuffer(f.read(8), dtype=np.int32)
            if magic[1] == 0x08: # Labels
                return np.frombuffer(f.read(), dtype=np.uint8)
            elif magic[1] == 0x0d: # Images
                rows, cols = np.frombuffer(f.read(8), dtype=np.int32)
                return np.frombuffer(f.read(), dtype=np.uint8).reshape(num[1], 1, rows[1], cols[1])
    
    # Download files if they don't exist
    base_url = "http://yann.lecun.com/exdb/mnist/"
    filenames = {
        "X_train": "train-images-idx3-ubyte.gz", "y_train": "train-labels-idx1-ubyte.gz",
        "X_test": "t10k-images-idx3-ubyte.gz", "y_test": "t10k-labels-idx1-ubyte.gz"
    }
    for key, filename in filenames.items():
        if not os.path.exists(filename):
            print(f"Downloading {filename}...")
            r = requests.get(base_url + filename)
            with open(filename, 'wb') as f:
                f.write(r.content)

    X_train = parse(filenames['X_train'])
    y_train = parse(filenames['y_train'])
    X_test = parse(filenames['X_test'])
    y_test = parse(filenames['y_test'])
    
    # Normalize and convert to float
    return X_train/255.0, y_train, X_test/255.0, y_test

# ==============================================================================
#  Section 1: Core Autodiff Engine (Node Class)
# ==============================================================================
# The full Node class from the previous step is assumed to be here.
# For brevity, only the class definition is shown.
class Node:
    # ... (Full implementation from previous step)
    def __init__(self, value, parents=(), op=''):
        self.value = np.array(value, dtype=float); self.parents = parents; self.op = op
        self.grad = None; self._backward = lambda: None
    def _ensure(self, other):
        return other if isinstance(other, Node) else Node(np.array(other, dtype=float))
    def __add__(self, other):
        other = self._ensure(other); out = Node(self.value + other.value, (self, other), '+')
        def _backward():
            if out.grad is None: return
            self.grad += _sum_to_shape(out.grad, self.value.shape)
            other.grad += _sum_to_shape(out.grad, other.value.shape)
        out._backward = _backward; return out
    def __mul__(self, other):
        other = self._ensure(other); out = Node(self.value * other.value, (self, other), '*')
        def _backward():
            if out.grad is None: return
            self.grad += _sum_to_shape(out.grad * other.value, self.value.shape)
            other.grad += _sum_to_shape(out.grad * self.value, other.value.shape)
        out._backward = _backward; return out
    def __pow__(self, power):
        assert isinstance(power, (int, float)); out = Node(self.value ** power, (self,), f'**{power}')
        def _backward():
            if out.grad is None: return
            self.grad += (power * self.value**(power-1)) * out.grad
        out._backward = _backward; return out
    def __matmul__(self, other):
        other = self._ensure(other); out = Node(self.value @ other.value, (self, other), '@')
        def _backward():
            if out.grad is None: return
            A, B, G = self.value, other.value, out.grad
            self.grad += _sum_to_shape(G @ B.T, self.value.shape)
            other.grad += _sum_to_shape(A.T @ G, other.value.shape)
        out._backward = _backward; return out
    def relu(self):
        out = Node(np.maximum(0, self.value), (self,), 'relu')
        def _backward():
            if out.grad is None: return
            self.grad += (self.value > 0) * out.grad
        out._backward = _backward; return out
    def log(self):
        out = Node(np.log(self.value), (self,), 'log')
        def _backward():
            if out.grad is None: return
            self.grad += (1 / self.value) * out.grad
        out._backward = _backward; return out
    def sum(self, axis=None, keepdims=False):
        out = Node(self.value.sum(axis=axis, keepdims=keepdims), (self,), 'sum')
        def _backward():
            if out.grad is None: return
            grad = out.grad
            if not keepdims and axis is not None:
                grad = np.expand_dims(out.grad, axis)
            self.grad += np.broadcast_to(grad, self.value.shape)
        out._backward = _backward; return out
    def exp(self):
        out = Node(np.exp(self.value), (self,), 'exp')
        def _backward():
            if out.grad is None: return
            self.grad += out.value * out.grad
        out._backward = _backward; return out
    def softmax(self, axis=-1):
        shifted_self = self + (self.value.max(axis=axis, keepdims=True) * -1)
        exp_x = shifted_self.exp()
        sum_exp_x = exp_x.sum(axis=axis, keepdims=True)
        return exp_x * (sum_exp_x**-1)
    def backward(self):
        topo, visited = [], set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for p in v.parents: build_topo(p)
                topo.append(v)
        build_topo(self)
        for v in topo: v.grad = np.zeros_like(v.value)
        self.grad = np.ones_like(self.value)
        for v in reversed(topo): v._backward()

# ==============================================================================
#  Section 2: The Complete 'nn' Module
# ==============================================================================
class nn:
    class Module:
        def parameters(self): yield from []
        def __call__(self, *args, **kwargs): return self.forward(*args, **kwargs)
        def zero_grad(self):
            for p in self.parameters(): p.grad = np.zeros_like(p.value)

    class Linear(Module):
        def __init__(self, in_features, out_features):
            self.weight = Node(np.random.randn(in_features, out_features) * 0.01)
            self.bias = Node(np.zeros(out_features))
        def forward(self, x): return x @ self.weight + self.bias
        def parameters(self): yield from [self.weight, self.bias]

    class Conv2d(Module):
        # ... (Full implementation from previous step)
        def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
            self.in_channels, self.out_channels = in_channels, out_channels
            self.kernel_size, self.stride, self.padding = kernel_size, stride, padding
            w_shape = (out_channels, in_channels, kernel_size, kernel_size)
            self.weight = Node(np.random.randn(*w_shape) * np.sqrt(2. / (in_channels * kernel_size**2)))
            self.bias = Node(np.zeros(out_channels))
        def parameters(self): yield from [self.weight, self.bias]
        def forward(self, x_node):
            x = x_node.value; N, C_in, H_in, W_in = x.shape
            H_out = (H_in + 2 * self.padding - self.kernel_size) // self.stride + 1
            W_out = (W_in + 2 * self.padding - self.kernel_size) // self.stride + 1
            X_col = self.im2col(x); W_col = self.weight.value.reshape(self.out_channels, -1)
            b_col = self.bias.value.reshape(-1, 1); out_col = W_col @ X_col + b_col
            out = out_col.reshape(self.out_channels, H_out, W_out, N).transpose(3, 0, 1, 2)
            output_node = Node(out, parents=(x_node, self.weight, self.bias), op='conv2d')
            def _backward():
                if output_node.grad is None: return
                dout_col = output_node.grad.transpose(1, 2, 3, 0).reshape(self.out_channels, -1)
                d_weight_col = dout_col @ X_col.T
                self.weight.grad += d_weight_col.reshape(self.weight.value.shape)
                self.bias.grad += np.sum(dout_col, axis=1)
                dW_col = W_col.T @ dout_col
                x_node.grad += self.col2im(dW_col, x.shape)
            output_node._backward = _backward; return output_node
        def im2col(self, x):
            N, C, H, W = x.shape; H_out = (H + 2 * self.padding - self.kernel_size) // self.stride + 1
            W_out = (W + 2 * self.padding - self.kernel_size) // self.stride + 1
            x_padded = np.pad(x, ((0,0),(0,0),(self.padding,self.padding),(self.padding,self.padding)),'constant')
            cols = np.zeros((C * self.kernel_size * self.kernel_size, N * H_out * W_out))
            for h in range(H_out):
                for w in range(W_out):
                    patch = x_padded[:, :, h*self.stride:h*self.stride+self.kernel_size, w*self.stride:w*self.stride+self.kernel_size]
                    cols[:, (h*W_out+w)*N:(h*W_out+w+1)*N] = patch.reshape(C*self.kernel_size*self.kernel_size, N)
            return cols
        def col2im(self, cols, x_shape):
            N, C, H, W = x_shape; H_padded, W_padded = H + 2 * self.padding, W + 2 * self.padding
            x_padded = np.zeros((N, C, H_padded, W_padded))
            H_out = (H + 2 * self.padding - self.kernel_size) // self.stride + 1
            W_out = (W + 2 * self.padding - self.kernel_size) // self.stride + 1
            cols = cols.reshape(C * self.kernel_size * self.kernel_size, H_out * W_out, N).transpose(2,0,1)
            for h in range(H_out):
                for w in range(W_out):
                    patch = cols[:, :, h*W_out+w].reshape(N, C, self.kernel_size, self.kernel_size)
                    x_padded[:, :, h*self.stride:h*self.stride+self.kernel_size, w*self.stride:w*self.stride+self.kernel_size] += patch
            return x_padded[:, :, self.padding:self.padding+H, self.padding:self.padding+W]

    class MaxPool2d(Module):
        def __init__(self, kernel_size, stride=None):
            self.kernel_size = kernel_size; self.stride = stride if stride is not None else kernel_size
            self.cache = {}
        def forward(self, x_node):
            x = x_node.value; N, C, H, W = x.shape
            H_out = (H - self.kernel_size) // self.stride + 1; W_out = (W - self.kernel_size) // self.stride + 1
            output = np.zeros((N, C, H_out, W_out)); max_indices = np.zeros_like(output, dtype=int)
            for h in range(H_out):
                for w in range(W_out):
                    window = x[:, :, h*self.stride:h*self.stride+self.kernel_size, w*self.stride:w*self.stride+self.kernel_size]
                    output[:,:,h,w] = np.max(window, axis=(2,3))
            self.cache['input_shape'] = x.shape
            output_node = Node(output, parents=(x_node,), op='maxpool2d')
            def _backward():
                if output_node.grad is None: return
                dx = np.zeros(self.cache['input_shape']); dout = output_node.grad
                x_val = x_node.value
                for n in range(N):
                    for c in range(C):
                        for h in range(H_out):
                            for w in range(W_out):
                                window = x_val[n, c, h*self.stride:h*self.stride+self.kernel_size, w*self.stride:w*self.stride+self.kernel_size]
                                mask = (window == np.max(window))
                                dx[n, c, h*self.stride:h*self.stride+self.kernel_size, w*self.stride:w*self.stride+self.kernel_size] += mask * dout[n,c,h,w]
                x_node.grad += dx
            output_node._backward = _backward; return output_node

    class Flatten(Module):
        def __init__(self): self.cache = {}
        def forward(self, x_node):
            x = x_node.value; self.cache['input_shape'] = x.shape
            output_node = Node(x.reshape(x.shape[0], -1), parents=(x_node,), op='flatten')
            def _backward():
                if output_node.grad is None: return
                x_node.grad += output_node.grad.reshape(self.cache['input_shape'])
            output_node._backward = _backward; return output_node

    class ReLU(Module):
        def forward(self, x): return x.relu()

    class Sequential(Module):
        def __init__(self, *layers): self.layers = layers
        def forward(self, x):
            for layer in self.layers: x = layer(x)
            return x
        def parameters(self):
            for layer in self.layers: yield from layer.parameters()

# ==============================================================================
#  Section 3: The Grand Finale - Training on MNIST
# ==============================================================================

def cross_entropy_loss(y_pred_probs, y_true_indices):
    """Calculates cross-entropy loss for a batch."""
    batch_size = y_pred_probs.value.shape[0]
    # Get the probabilities corresponding to the true class for each sample
    true_class_probs = Node(y_pred_probs.value[np.arange(batch_size), y_true_indices])
    # Clip to avoid log(0)
    clipped_probs = Node(np.clip(true_class_probs.value, 1e-10, 1.0))
    # Calculate log loss
    log_loss = clipped_probs.log() * -1
    return log_loss.sum() * (1 / batch_size)

if __name__ == "__main__":
    # 1. Load Data
    X_train, y_train, X_test, y_test = fetch_mnist()
    print("MNIST data loaded.")
    
    # 2. Build the LeNet-5 inspired model
    np.random.seed(0)
    model = nn.Sequential(
        nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2), # -> (N, 6, 28, 28)
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2), # -> (N, 6, 14, 14)
        nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5), # -> (N, 16, 10, 10)
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2), # -> (N, 16, 5, 5)
        nn.Flatten(), # -> (N, 16*5*5 = 400)
        nn.Linear(in_features=16*5*5, out_features=120),
        nn.ReLU(),
        nn.Linear(in_features=120, out_features=84),
        nn.ReLU(),
        nn.Linear(in_features=84, out_features=10) # 10 classes for digits 0-9
    )
    
    # 3. Training
    print("Starting CNN training on MNIST...")
    learning_rate = 0.01
    batch_size = 64
    epochs = 3

    params = list(model.parameters())
    
    for epoch in range(epochs):
        epoch_start_time = time.time()
        # Mini-batch training
        permutation = np.random.permutation(X_train.shape[0])
        for i in range(0, X_train.shape[0], batch_size):
            indices = permutation[i:i+batch_size]
            X_batch, y_batch = X_train[indices], y_train[indices]
            
            # Forward pass
            logits = model(Node(X_batch))
            probabilities = logits.softmax()
            
            # Loss calculation
            loss = cross_entropy_loss(probabilities, y_batch)
            
            # Backward pass and optimization
            model.zero_grad()
            loss.backward()
            
            for p in params:
                p.value -= learning_rate * p.grad
        
        # 4. Evaluation at the end of each epoch
        print(f"\n--- Epoch {epoch+1}/{epochs} Completed ---")
        print(f"Epoch time: {time.time() - epoch_start_time:.2f}s")
        print(f"Final batch loss: {loss.value.item():.4f}")
        
        # Evaluate on a subset of the test set for speed
        test_sample_size = 500
        test_logits = model(Node(X_test[:test_sample_size]))
        test_probs = test_logits.softmax()
        y_pred_indices = np.argmax(test_probs.value, axis=1)
        accuracy = np.mean(y_pred_indices == y_test[:test_sample_size])
        print(f"Test Accuracy on {test_sample_size} samples: {accuracy * 100:.2f}%")

    print("\n--- TRAINING COMPLETE ---")
    
    # Final, more thorough evaluation
    print("\nPerforming final evaluation on 2000 test samples...")
    final_logits = model(Node(X_test[:2000]))
    final_probs = final_logits.softmax()
    final_preds = np.argmax(final_probs.value, axis=1)
    final_accuracy = np.mean(final_preds == y_test[:2000])
    
    print("\n======================================")
    print("          FINAL CNN RESULTS")
    print("======================================")
    print(f"Final Test Accuracy: {final_accuracy * 100:.2f}%")
    if final_accuracy > 0.95:
        print("🎉🎉🎉 PHENOMENAL SUCCESS! Our custom CNN is highly accurate! 🎉🎉🎉")
    else:
        print("Great work! The model is learning correctly. Further tuning can improve accuracy.")

