"""
Multi-Agent Strategy Discovery System
====================================

This system implements autonomous strategy discovery for 4 collaborative agents
using enhanced exploration, learned subgoal optimization, and dueling Q-networks.

Key Innovation: Agents learn exploration strategies through interaction and
develop emergent coordination without pre-programmed behaviors.
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
import math
from collections import defaultdict, deque
from typing import List, Tuple, Dict, Optional, Any
import matplotlib.pyplot as plt

class AdaptiveExplorationStrategy:
    """
    Ø®Ø·ÙˆØ© 1: Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ù…Ø¹Ø²Ø²Ø©
    
    ÙŠØªÙƒÙŠÙ ÙƒÙ„ ÙˆÙƒÙŠÙ„ Ù…Ø¹ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆÙŠØ·ÙˆØ± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ø³ØªÙƒØ´Ø§Ù Ù…Ø®ØµØµØ©
    Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¬Ø§Ø­Ø§Øª ÙˆØ§Ù„ÙØ´Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚.
    """
    
    def __init__(self, agent_id: int, learning_rate: float = 0.01):
        self.agent_id = agent_id
        self.learning_rate = learning_rate
        
        # Ù…ØªØªØ¨Ø¹Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø´Ø®ØµÙŠØ©
        self.success_history = deque(maxlen=100)
        self.exploration_effectiveness = defaultdict(float)
        self.problem_complexity_memory = {}
        
        # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„ØªÙƒÙŠÙÙŠØ©
        self.base_epsilon = 0.1
        self.curiosity_factor = 0.2
        self.social_learning_factor = 0.15  # Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†
        
        # Ø¢Ù„ÙŠØ§Øª Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ÙˆÙ‚ÙˆØ¹ ÙÙŠ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        self.stagnation_detector = StagnationDetector()
        self.escape_strategies = EscapeStrategyLibrary()
        
    def compute_exploration_rate(self, current_state: Dict, 
                                other_agents_performance: List[Dict]) -> float:
        """
        Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰:
        1. ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        2. Ù…Ø¯Ù‰ Ù†Ø¬Ø§Ø­ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
        3. Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† (Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ)
        """
        # ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
        problem_complexity = self._assess_problem_complexity(current_state)
        
        # ØªÙ‚ÙŠÙŠÙ… ÙØ¹Ø§Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø³Ø§Ø¨Ù‚
        historical_effectiveness = self._get_historical_effectiveness(problem_complexity)
        
        # Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†
        social_learning_bonus = self._compute_social_learning_bonus(other_agents_performance)
        
        # ÙƒØ´Ù Ø§Ù„Ø¬Ù…ÙˆØ¯ ÙˆØ§Ù„Ø­Ø§Ø¬Ø© Ù„Ù„Ù‡Ø±ÙˆØ¨
        stagnation_penalty = self.stagnation_detector.assess_stagnation(current_state)
        
        # Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„ØªÙƒÙŠÙÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        exploration_rate = (
            self.base_epsilon +
            problem_complexity * 0.3 +
            (1.0 - historical_effectiveness) * 0.4 +
            social_learning_bonus +
            stagnation_penalty
        )
        
        return min(0.8, max(0.05, exploration_rate))  # Ø­Ø¯ÙˆØ¯ Ø¢Ù…Ù†Ø©
    
    def _assess_problem_complexity(self, state: Dict) -> float:
        """ØªÙ‚ÙŠÙŠÙ… ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""
        gap = abs(state['target'] - state['current'])
        constraint_density = len(state['forbidden_states']) / max(1, gap)
        time_pressure = state['step'] / state['max_steps']
        
        # Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø±ÙƒØ¨
        complexity = (
            min(1.0, gap / 500) * 0.4 +           # Ø­Ø¬Ù… Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
            min(1.0, constraint_density * 10) * 0.4 +  # ÙƒØ«Ø§ÙØ© Ø§Ù„Ù‚ÙŠÙˆØ¯
            time_pressure * 0.2                        # Ø¶ØºØ· Ø§Ù„ÙˆÙ‚Øª
        )
        
        return complexity
    
    def _get_historical_effectiveness(self, complexity_level: float) -> float:
        """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙØ¹Ø§Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ÙÙŠ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù‡Ø°Ø§"""
        complexity_key = int(complexity_level * 10)  # ØªØ¬Ù…ÙŠØ¹ Ù„Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
        
        if complexity_key in self.exploration_effectiveness:
            return self.exploration_effectiveness[complexity_key]
        else:
            return 0.5  # Ù‚ÙŠÙ…Ø© Ù…Ø­Ø§ÙŠØ¯Ø© Ù„Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    
    def _compute_social_learning_bonus(self, other_agents_performance: List[Dict]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…ÙƒØ§ÙØ£Ø© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ù†Ø¬Ø§Ø­ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†"""
        if not other_agents_performance:
            return 0.0
        
        # Ù…Ø¹Ø¯Ù„ Ù†Ø¬Ø§Ø­ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†
        others_success_rate = np.mean([perf['recent_success_rate'] 
                                     for perf in other_agents_performance])
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¢Ø®Ø±ÙˆÙ† ÙŠÙ†Ø¬Ø­ÙˆÙ† Ø£ÙƒØ«Ø±ØŒ Ø²Ø¯ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ù„Ù„ØªØ¹Ù„Ù… Ù…Ù†Ù‡Ù…
        my_success_rate = np.mean(self.success_history) if self.success_history else 0.5
        
        if others_success_rate > my_success_rate + 0.1:
            return self.social_learning_factor
        else:
            return 0.0
    
    def select_exploration_action(self, available_actions: List[float], 
                                current_state: Dict) -> float:
        """Ø§Ø®ØªÙŠØ§Ø± Ø­Ø±ÙƒØ© Ø§Ø³ØªÙƒØ´Ø§ÙÙŠØ© Ø°ÙƒÙŠØ©"""
        
        # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        strategies = [
            self._curiosity_driven_action,
            self._diversity_seeking_action,
            self._escape_oriented_action,
            self._social_imitation_action
        ]
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ
        if self.stagnation_detector.is_stuck():
            strategy = self._escape_oriented_action
        elif self._should_imitate_others():
            strategy = self._social_imitation_action
        else:
            strategy = random.choice(strategies[:2])  # ÙØ¶ÙˆÙ„ Ø£Ùˆ ØªÙ†ÙˆÙŠØ¹
        
        return strategy(available_actions, current_state)
    
    def _curiosity_driven_action(self, actions: List[float], state: Dict) -> float:
        """Ø§Ø®ØªÙŠØ§Ø± Ø­Ø±ÙƒØ© ØªØ¹Ø¸Ù… Ø§Ù„ÙØ¶ÙˆÙ„ (Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ ØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙƒØ´ÙØ©)"""
        curiosity_scores = []
        
        for action in actions:
            next_pos = state['current'] + action
            # Ù…Ù†Ø­ Ù†Ù‚Ø§Ø· Ø£Ø¹Ù„Ù‰ Ù„Ù„Ù…ÙˆØ§Ø¶Ø¹ ØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙƒØ´ÙØ©
            visit_count = self.problem_complexity_memory.get(next_pos, 0)
            curiosity_score = 1.0 / (visit_count + 1)
            curiosity_scores.append((action, curiosity_score))
        
        return max(curiosity_scores, key=lambda x: x[1])[0]
    
    def _diversity_seeking_action(self, actions: List[float], state: Dict) -> float:
        """Ø§Ø®ØªÙŠØ§Ø± Ø­Ø±ÙƒØ© ØªØ²ÙŠØ¯ Ø§Ù„ØªÙ†ÙˆÙŠØ¹ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©"""
        # ØªØ¬Ù†Ø¨ ØªÙƒØ±Ø§Ø± Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        recent_actions = getattr(self, 'recent_actions', [])
        
        for action in actions:
            if action not in recent_actions[-3:]:  # ØªØ¬Ù†Ø¨ Ø¢Ø®Ø± 3 Ø­Ø±ÙƒØ§Øª
                return action
        
        return random.choice(actions)  # Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ÙƒÙ„Ù‡Ø§ Ù…ÙƒØ±Ø±Ø©
    
    def _escape_oriented_action(self, actions: List[float], state: Dict) -> float:
        """Ø§Ø®ØªÙŠØ§Ø± Ø­Ø±ÙƒØ© Ù„Ù„Ù‡Ø±ÙˆØ¨ Ù…Ù† Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ø­Ù„ÙŠØ©"""
        return self.escape_strategies.get_escape_action(actions, state)
    
    def _social_imitation_action(self, actions: List[float], state: Dict) -> float:
        """Ù…Ø­Ø§ÙˆÙ„Ø© ØªÙ‚Ù„ÙŠØ¯ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ù†Ø§Ø¬Ø­ÙŠÙ†"""
        # Ø³ÙŠØªÙ… ØªÙ†ÙÙŠØ°Ù‡Ø§ Ù…Ù† Ø®Ù„Ø§Ù„ Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¨ÙŠÙ† Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡
        return random.choice(actions)  # Ù…Ø¨Ø³Ø· Ù„Ù„Ø¢Ù†
    
    def update_from_experience(self, state: Dict, action: float, 
                             reward: float, success: bool):
        """ØªØ­Ø¯ÙŠØ« Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        
        # ØªØ­Ø¯ÙŠØ« ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø¬Ø§Ø­
        self.success_history.append(1.0 if success else 0.0)
        
        # ØªØ­Ø¯ÙŠØ« ÙØ¹Ø§Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
        complexity = self._assess_problem_complexity(state)
        complexity_key = int(complexity * 10)
        
        # ØªØ¹Ù„Ù… ØªØ¯Ø±ÙŠØ¬ÙŠ Ù„Ù„ÙØ¹Ø§Ù„ÙŠØ©
        current_effectiveness = self.exploration_effectiveness[complexity_key]
        reward_signal = 1.0 if success else -0.1
        
        self.exploration_effectiveness[complexity_key] = (
            current_effectiveness * (1 - self.learning_rate) +
            reward_signal * self.learning_rate
        )
        
        # ØªØ­Ø¯ÙŠØ« Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹
        self.problem_complexity_memory[state['current']] = \
            self.problem_complexity_memory.get(state['current'], 0) + 1


class LearnedSubgoalOptimizer:
    """
    Ø®Ø·ÙˆØ© 2: Ù…Ø­Ø³Ù† Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø§Ù„Ù…ÙƒØªØ³Ø¨Ø©
    
    ÙŠØªØ¹Ù„Ù… ÙƒÙ„ ÙˆÙƒÙŠÙ„ ÙƒÙŠÙÙŠØ© Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø§Ù„Ø£Ù…Ø«Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰:
    - Ø§Ù„Ù†Ø¬Ø§Ø­Ø§Øª ÙˆØ§Ù„ÙØ´Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚
    - ØªØ´Ø§Ø±Ùƒ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ø¹ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†
    - Ø§Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
    """
    
    def __init__(self, agent_id: int):
        self.agent_id = agent_id
        
        # Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© Ù„ØªØ¹Ù„Ù… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ÙØ±Ø¹ÙŠØ©
        self.subgoal_network = SubgoalSelectionNetwork()
        self.optimizer = optim.Adam(self.subgoal_network.parameters(), lr=0.001)
        
        # Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†Ø§Ø¬Ø­Ø©
        self.success_patterns = PatternLibrary()
        self.failure_patterns = PatternLibrary()
        
        # Ø¢Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ
        self.social_knowledge = SharedKnowledgeBase()
        
    def discover_optimal_subgoals(self, current: float, target: float,
                                forbidden_states: set, 
                                other_agents_knowledge: List[Dict]) -> List[float]:
        """
        Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø§Ù„Ù…Ø«Ù„Ù‰ Ù…Ù† Ø®Ù„Ø§Ù„:
        1. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù…Ø´ÙƒÙ„Ø©
        2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø©
        3. Ø¯Ù…Ø¬ Ù…Ø¹Ø±ÙØ© Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†
        """
        
        # ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
        problem_features = self._analyze_problem_structure(current, target, forbidden_states)
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· Ù…Ø´Ø§Ø¨Ù‡Ø© Ù†Ø§Ø¬Ø­Ø©
        similar_successes = self.success_patterns.find_similar(problem_features)
        
        # Ø¯Ù…Ø¬ Ù…Ø¹Ø±ÙØ© Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†
        social_insights = self._integrate_social_knowledge(other_agents_knowledge, problem_features)
        
        # ØªÙˆÙ„ÙŠØ¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª
        candidate_decompositions = self._generate_candidate_decompositions(
            current, target, problem_features, similar_successes, social_insights
        )
        
        # ØªÙ‚ÙŠÙŠÙ… ÙˆØ§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ ØªØ­Ù„ÙŠÙ„
        best_decomposition = self._evaluate_and_select_best(candidate_decompositions, problem_features)
        
        return best_decomposition
    
    def _analyze_problem_structure(self, current: float, target: float, 
                                 forbidden_states: set) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ù„Ù„Ù…Ø´ÙƒÙ„Ø©"""
        gap = abs(target - current)
        
        # ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙˆØ¯
        constraint_clusters = self._identify_constraint_clusters(current, target, forbidden_states)
        constraint_density = len(forbidden_states) / max(1, gap)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ¹ÙˆØ¨Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
        expected_difficulty = self._estimate_difficulty(gap, constraint_density, constraint_clusters)
        
        return {
            'gap_size': gap,
            'constraint_density': constraint_density,
            'constraint_clusters': constraint_clusters,
            'expected_difficulty': expected_difficulty,
            'problem_signature': hash((gap // 50, len(constraint_clusters)))
        }
    
    def _generate_candidate_decompositions(self, current: float, target: float,
                                         problem_features: Dict,
                                         similar_successes: List[Dict],
                                         social_insights: List[Dict]) -> List[List[float]]:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø©"""
        candidates = []
        
        # 1. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†Ø§Ø¬Ø­Ø©
        for success in similar_successes:
            adapted = self._adapt_successful_pattern(current, target, success['subgoals'])
            candidates.append(adapted)
        
        # 2. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©
        for insight in social_insights:
            social_decomposition = self._apply_social_insight(current, target, insight)
            candidates.append(social_decomposition)
        
        # 3. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
        neural_decomposition = self._neural_subgoal_generation(current, target, problem_features)
        candidates.append(neural_decomposition)
        
        # 4. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒÙŠÙÙŠ Ø§Ù„Ø°ÙƒÙŠ
        adaptive_decomposition = self._adaptive_decomposition(current, target, problem_features)
        candidates.append(adaptive_decomposition)
        
        return candidates
    
    def _neural_subgoal_generation(self, current: float, target: float, 
                                 problem_features: Dict) -> List[float]:
        """Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ÙØ±Ø¹ÙŠØ©"""
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ©
        network_input = self._prepare_network_input(current, target, problem_features)
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ÙØ±Ø¹ÙŠØ©
        with torch.no_grad():
            subgoal_predictions = self.subgoal_network(network_input)
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø¥Ù„Ù‰ Ø£Ù‡Ø¯Ø§Ù ÙØ±Ø¹ÙŠØ© Ø­Ù‚ÙŠÙ‚ÙŠØ©
        subgoals = self._decode_neural_output(current, target, subgoal_predictions)
        
        return subgoals
    
    def learn_from_outcome(self, problem_features: Dict, subgoals_used: List[float],
                          success: bool, efficiency: float, step_count: int):
        """Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¬Ø±Ø¨Ø©"""
        
        # ØªØ­Ø¯ÙŠØ« Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø·
        outcome_data = {
            'subgoals': subgoals_used,
            'success': success,
            'efficiency': efficiency,
            'step_count': step_count,
            'timestamp': time.time()
        }
        
        if success and efficiency > 0.7:
            self.success_patterns.add_pattern(problem_features, outcome_data)
        else:
            self.failure_patterns.add_pattern(problem_features, outcome_data)
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
        if len(self.success_patterns.patterns) % 10 == 0:  # ÙƒÙ„ 10 ØªØ¬Ø§Ø±Ø¨
            self._retrain_neural_network()


class DuelingQNetwork(nn.Module):
    """
    Ø®Ø·ÙˆØ© 3: Ø¨Ù†ÙŠØ© Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù‡Ø±Ù…ÙŠØ© (Dueling Q-Network)
    
    ØªÙØµÙ„ Ø¨ÙŠÙ† ØªÙ‚ÙŠÙŠÙ… Ù‚ÙŠÙ…Ø© Ø§Ù„Ø­Ø§Ù„Ø© ÙˆÙ…Ø²Ø§ÙŠØ§ Ø§Ù„Ø£ÙØ¹Ø§Ù„ØŒ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰:
    - ØªØ¹Ù„Ù… Ø£ÙØ¶Ù„ Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø§Øª
    - Ù‚Ø±Ø§Ø±Ø§Øª Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
    - ØªÙ‚Ø§Ø±Ø¨ Ø£Ø³Ø±Ø¹ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    """
    
    def __init__(self, state_dim: int = 18, action_dim: int = 3, hidden_dim: int = 512):
        super().__init__()
        
        # Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø´ØªØ±Ùƒ Ø§Ù„Ù…Ø­Ø³Ù†
        self.feature_extractor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),  # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2)
        )
        
        # ÙØ±Ø¹ ØªÙ‚ÙŠÙŠÙ… Ù‚ÙŠÙ…Ø© Ø§Ù„Ø­Ø§Ù„Ø© V(s)
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, 1)  # Ù‚ÙŠÙ…Ø© ÙˆØ§Ø­Ø¯Ø© Ù„Ù„Ø­Ø§Ù„Ø©
        )
        
        # ÙØ±Ø¹ Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ø£ÙØ¹Ø§Ù„ A(s,a)
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, action_dim)  # Ù…ÙŠØ²Ø© Ù„ÙƒÙ„ ÙØ¹Ù„
        )
        
        # Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù„Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ§Ù‹
        self.attention_mechanism = AttentionLayer(hidden_dim // 2)
        
    def forward(self, state_features: torch.Tensor) -> torch.Tensor:
        """
        Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ù„Ù€ Dueling Q-Network
        
        Q(s,a) = V(s) + A(s,a) - mean(A(s,Â·))
        """
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        features = self.feature_extractor(state_features)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø¢Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        attended_features = self.attention_mechanism(features)
        
        # Ø­Ø³Ø§Ø¨ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø­Ø§Ù„Ø©
        state_value = self.value_stream(attended_features)
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ø£ÙØ¹Ø§Ù„
        action_advantages = self.advantage_stream(attended_features)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù‚ÙŠÙ…Ø© ÙˆØ§Ù„Ù…Ø²Ø§ÙŠØ§ (Dueling Architecture)
        # Q(s,a) = V(s) + [A(s,a) - mean(A(s,Â·))]
        mean_advantage = action_advantages.mean(dim=1, keepdim=True)
        q_values = state_value + (action_advantages - mean_advantage)
        
        return q_values


class MultiAgentCoordinationSystem:
    """
    Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡
    
    ÙŠØ¯ÙŠØ± Ø§Ù„ØªÙØ§Ø¹Ù„ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø´ØªØ±Ùƒ Ø¨ÙŠÙ† Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø£Ø±Ø¨Ø¹Ø©
    """
    
    def __init__(self, num_agents: int = 4):
        self.num_agents = num_agents
        self.agents = []
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ù…Ø¹ ØªØ®ØµØµØ§Øª Ù…Ø®ØªÙ„ÙØ©
        specializations = ['explorer', 'optimizer', 'conservative', 'adaptive']
        
        for i in range(num_agents):
            agent = AutonomousDiscoveryAgent(
                agent_id=i,
                specialization=specializations[i],
                actions=[1, 3, 5]
            )
            self.agents.append(agent)
        
        # Ù†Ø¸Ø§Ù… Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ù…Ø¹Ø±ÙØ©
        self.knowledge_sharing_system = KnowledgeSharingHub()
        
        # Ù…Ø¯ÙŠØ± Ø§Ù„ØªÙ†Ø³ÙŠÙ‚
        self.coordination_manager = CoordinationManager(self.agents)
        
    def collaborative_training_session(self, training_problems: List[Dict],
                                     episodes_per_problem: int = 50) -> Dict:
        """
        Ø¬Ù„Ø³Ø© ØªØ¯Ø±ÙŠØ¨ ØªØ¹Ø§ÙˆÙ†ÙŠØ© Ø­ÙŠØ« ÙŠØªØ¹Ù„Ù… Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ù…Ù† Ø¨Ø¹Ø¶Ù‡Ù… Ø§Ù„Ø¨Ø¹Ø¶
        """
        
        training_results = {
            'individual_performance': {i: [] for i in range(self.num_agents)},
            'collaborative_discoveries': [],
            'emergent_strategies': [],
            'knowledge_transfer_events': []
        }
        
        for problem_idx, problem in enumerate(training_problems):
            print(f"\nğŸ¯ Problem {problem_idx + 1}: Target {problem['target']}")
            
            # ÙƒÙ„ ÙˆÙƒÙŠÙ„ ÙŠØ¬Ø±Ø¨ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
            problem_results = []
            
            for episode in range(episodes_per_problem):
                episode_results = []
                
                for agent in self.agents:
                    # Ø¬Ù…Ø¹ Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ
                    others_performance = [a.get_recent_performance() 
                                        for a in self.agents if a != agent]
                    
                    # Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
                    result = agent.solve_with_discovery(
                        problem=problem,
                        others_performance=others_performance
                    )
                    
                    episode_results.append(result)
                    training_results['individual_performance'][agent.agent_id].append(result)
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØªØ¨Ø§Ø¯Ù„ Ø§Ù„Ù…Ø¹Ø±ÙØ©
                knowledge_transfer = self.knowledge_sharing_system.process_episode_results(
                    episode_results, self.agents
                )
                
                if knowledge_transfer:
                    training_results['knowledge_transfer_events'].append(knowledge_transfer)
                
                # ÙƒØ´Ù Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù†Ø§Ø´Ø¦Ø©
                emergent_strategies = self.coordination_manager.analyze_emergent_strategies(
                    episode_results
                )
                
                if emergent_strategies:
                    training_results['emergent_strategies'].extend(emergent_strategies)
        
        return training_results
    
    def evaluate_collaborative_performance(self, test_problems: List[Dict]) -> Dict:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ¹Ø§ÙˆÙ†ÙŠ Ù„Ù„ÙˆÙƒÙ„Ø§Ø¡"""
        
        evaluation_results = {
            'individual_success_rates': {},
            'best_agent_per_problem': {},
            'collaborative_improvement': {},
            'strategy_diversity': {},
            'knowledge_utilization': {}
        }
        
        for problem in test_problems:
            problem_results = []
            
            # ÙƒÙ„ ÙˆÙƒÙŠÙ„ ÙŠØ­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨Ø´ÙƒÙ„ Ù…Ø³ØªÙ‚Ù„
            for agent in self.agents:
                others_knowledge = [a.share_knowledge() for a in self.agents if a != agent]
                
                result = agent.solve_with_discovery(
                    problem=problem,
                    others_performance=others_knowledge,
                    training=False
                )
                
                problem_results.append({
                    'agent_id': agent.agent_id,
                    'result': result,
                    'strategies_used': agent.get_strategies_used(),
                    'knowledge_sources': agent.get_knowledge_sources()
                })
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            self._analyze_problem_results(problem, problem_results, evaluation_results)
        
        return evaluation_results


class AutonomousDiscoveryAgent:
    """
    Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª
    
    ÙŠØ¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø«Ù„Ø§Ø«Ø©:
    1. Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„ØªÙƒÙŠÙÙŠ
    2. ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø§Ù„Ù…ØªØ¹Ù„Ù…
    3. Dueling Q-Network
    """
    
    def __init__(self, agent_id: int, specialization: str, actions: List[float]):
        self.agent_id = agent_id
        self.specialization = specialization
        self.actions = actions
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.exploration_strategy = AdaptiveExplorationStrategy(agent_id)
        self.subgoal_optimizer = LearnedSubgoalOptimizer(agent_id)
        
        # Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        self.q_network = DuelingQNetwork(state_dim=18, action_dim=len(actions))
        self.target_network = DuelingQNetwork(state_dim=18, action_dim=len(actions))
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.0003)
        
        # Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©
        self.discovered_strategies = StrategyLibrary()
        self.performance_tracker = PerformanceTracker()
        
        # Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø®Ø¨Ø±Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        self.experience_replay = PrioritizedExperienceReplay(capacity=50000)
        
        # Ù…ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.episode_count = 0
        self.discovery_log = []
        
    def solve_with_discovery(self, problem: Dict, 
                           others_performance: List[Dict],
                           training: bool = True) -> Dict:
        """
        Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ù…Ø¹ Ø§ÙƒØªØ´Ø§Ù Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø©
        """
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
        current = problem['start'] 
        target = problem['target']
        forbidden_states = problem.get('forbidden_states', set())
        max_steps = problem.get('max_steps', int(abs(target - current) / max(self.actions) * 1.5))
        
        # Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ÙØ±Ø¹ÙŠØ© Ø¨Ø°ÙƒØ§Ø¡
        others_knowledge = [perf.get('knowledge', {}) for perf in others_performance]
        subgoals = self.subgoal_optimizer.discover_optimal_subgoals(
            current, target, forbidden_states, others_knowledge
        )
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø­Ù„ Ù…Ø¹ Ø§Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ØªÙƒÙŠÙÙŠ
        path = [current]
        step = 0
        strategies_used = []
        discoveries_made = []
        
        current_subgoal_idx = 0
        
        while step < max_steps and current != target:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„ÙØ±Ø¹ÙŠ Ø§Ù„Ø­Ø§Ù„ÙŠ
            working_target = (subgoals[current_subgoal_idx] 
                            if current_subgoal_idx < len(subgoals) 
                            else target)
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
            state_dict = {
                'current': current,
                'target': working_target,
                'step': step,
                'max_steps': max_steps,
                'forbidden_states': forbidden_states
            }
            
            exploration_rate = self.exploration_strategy.compute_exploration_rate(
                state_dict, others_performance
            )
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø­Ø±ÙƒØ©
            if training and random.random() < exploration_rate:
                # Ø§ÙƒØªØ´Ø§Ù Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©
                action = self.exploration_strategy.select_exploration_action(
                    self.actions, state_dict
                )
                strategy_type = 'exploration'
            else:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ÙƒØªØ³Ø¨Ø©
                action = self._neural_action_selection(state_dict)
                strategy_type = 'exploitation'
            
            # ØªÙ†ÙÙŠØ° Ø§Ù„Ø­Ø±ÙƒØ©
            next_current = current + action
            reward = self._compute_reward(current, next_current, working_target, 
                                        step, forbidden_states)
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø¨Ø±Ø©
            if training:
                self._store_experience(state_dict, action, reward, next_current, step + 1)
            
            # ØªØªØ¨Ø¹ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª
            strategies_used.append({
                'step': step,
                'strategy_type': strategy_type,
                'action': action,
                'exploration_rate': exploration_rate
            })
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø©
            current = next_current
            step += 1
            path.append(current)
            
            # ÙØ­Øµ Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„ÙØ±Ø¹ÙŠ
            if (current_subgoal_idx < len(subgoals) and 
                abs(current - subgoals[current_subgoal_idx]) < 0.5):
                current_subgoal_idx += 1
                
                # Ø§ÙƒØª