==============================================================================

ุงูุณูุฑูุจุช ุงููุงูู ูุงูููุงุฆู (ูุณุฎุฉ ูุญุณูุฉ ููุฏููุฉ): ุงููุนุฑูุฉ ุงูุญุงุณูุฉ

ุงููุคูููู: ุฃูุช ู Manus (ูุฑุงุฌุนุฉ: ChatGPT)

ุงูุชุงุฑูุฎ: 05 ุณุจุชูุจุฑ 2025

==============================================================================

import numpy as np import time from tensorflow.keras.datasets import mnist

----------------------------- ุฃุฏูุงุช ุฏุงุฎููุฉ ---------------------------------

def _sum_to_shape(grad, shape): """Reduce grad to shape by summing over extra/size-1 axes (for broadcasting). Returns an array with exactly shape. """ g = grad # sum leading axes while g.ndim > len(shape): g = g.sum(axis=0) # sum axes where target dim == 1 for i, s in enumerate(shape): if s == 1 and g.shape[i] != 1: g = g.sum(axis=i, keepdims=True) # final check/reshape if g.shape != shape: g = g.reshape(shape) return g

==============================================================================

ุงููุณู 1: ุงูููุงุฉ ุงูุฃุณุงุณูุฉ - ูุฆุฉ Node (ูุณุฎุฉ ููุญุณููุฉ)

==============================================================================

class Node: def init(self, value, parents=(), op=''): self.value = np.array(value, dtype=np.float32) self.parents = tuple(parents) self.op = op self.grad = None # default noop backward self._backward = lambda out_node: None

def _ensure(self, other):
    return other if isinstance(other, Node) else Node(np.array(other, dtype=np.float32))

def _accumulate(self, param, grad):
    if param.grad is None:
        param.grad = np.zeros_like(param.value)
    grad_reduced = _sum_to_shape(grad, param.value.shape)
    param.grad += grad_reduced

def __add__(self, other):
    other = self._ensure(other)
    out = Node(self.value + other.value, (self, other), '+')
    def _backward(out_node):
        g = out_node.grad
        self._accumulate(self, g)
        other._accumulate(other, g)
    out._backward = _backward
    return out

def __mul__(self, other):
    other = self._ensure(other)
    out = Node(self.value * other.value, (self, other), '*')
    def _backward(out_node):
        g = out_node.grad
        self._accumulate(self, other.value * g)
        other._accumulate(other, self.value * g)
    out._backward = _backward
    return out

def __matmul__(self, other):
    other = self._ensure(other)
    out = Node(self.value @ other.value, (self, other), '@')
    def _backward(out_node):
        g = out_node.grad
        # shapes: (N, A) @ (A, B) -> (N, B)
        # grad wrt self: g @ other.T
        self._accumulate(self, g @ other.value.T)
        # grad wrt other: self.T @ g
        other._accumulate(other, self.value.T @ g)
    out._backward = _backward
    return out

def __pow__(self, power):
    out = Node(self.value ** power, (self,), f'**{power}')
    def _backward(out_node):
        g = out_node.grad
        self._accumulate(self, (power * (self.value ** (power - 1))) * g)
    out._backward = _backward
    return out

def __neg__(self):
    return self * -1

def __sub__(self, other):
    other = self._ensure(other)
    return self + (other * -1)

def __truediv__(self, other):
    other = self._ensure(other)
    return self * (other ** -1)

def relu(self):
    out = Node(np.maximum(0, self.value), (self,), 'ReLU')
    def _backward(out_node):
        g = out_node.grad
        self._accumulate(self, (self.value > 0).astype(np.float32) * g)
    out._backward = _backward
    return out

def log(self):
    eps = 1e-10
    out = Node(np.log(np.maximum(self.value, eps)), (self,), 'log')
    def _backward(out_node):
        g = out_node.grad
        self._accumulate(self, (1.0 / np.maximum(self.value, eps)) * g)
    out._backward = _backward
    return out

def sum(self, axis=None, keepdims=False):
    out_val = self.value.sum(axis=axis, keepdims=keepdims)
    out = Node(out_val, (self,), 'sum')
    def _backward(out_node):
        g = out_node.grad
        if not keepdims and axis is not None:
            g = np.expand_dims(g, axis)
        self._accumulate(self, np.broadcast_to(g, self.value.shape))
    out._backward = _backward
    return out

def exp(self):
    val = np.exp(np.clip(self.value, -100, 100))
    out = Node(val, (self,), 'exp')
    def _backward(out_node):
        g = out_node.grad
        self._accumulate(self, val * g)
    out._backward = _backward
    return out

def reshape(self, *shape):
    out_val = self.value.reshape(*shape)
    out = Node(out_val, (self,), 'reshape')
    def _backward(out_node):
        g = out_node.grad
        self._accumulate(self, g.reshape(self.value.shape))
    out._backward = _backward
    return out

def build_topo(self):
    topo, visited = [], set()
    def build(v):
        if id(v) not in visited:
            visited.add(id(v))
            for p in v.parents: build(p)
            topo.append(v)
    build(self)
    return topo

def backward(self):
    topo = self.build_topo()
    # reset grads
    for v in topo:
        v.grad = None
    self.grad = np.ones_like(self.value)
    for v in reversed(topo):
        # ensure grad exists for node before backward
        if v.grad is None:
            v.grad = np.zeros_like(v.value)
        v._backward(v)
    return topo

==============================================================================

ุงููุณู 2: ูุญุฏุฉ ุจูุงุก ุงูุดุจูุงุช (nn)

==============================================================================

class nn: class Module: def train(self): self._is_training = True for layer in self.dict.values(): if isinstance(layer, nn.Module): layer.train() def eval(self): self._is_training = False for layer in self.dict.values(): if isinstance(layer, nn.Module): layer.eval() def parameters(self): for name, value in self.dict.items(): if isinstance(value, nn.Module): yield from value.parameters() elif isinstance(value, Node) and name != 'grad': yield value def call(self, *args, **kwargs): return self.forward(*args, **kwargs) def zero_grad(self): for p in self.parameters(): p.grad = None

class Linear(Module):
    def __init__(self, in_features, out_features):
        limit = np.sqrt(2 / in_features)
        self.weight = Node(np.random.randn(in_features, out_features) * limit)
        self.bias = Node(np.zeros(out_features))
    def forward(self, x):
        return x @ self.weight + self.bias
    def parameters(self): yield from [self.weight, self.bias]

class ReLU(Module):
    def forward(self, x): return x.relu()

class Flatten(Module):
    def forward(self, x):
        N = x.value.shape[0]
        return x.reshape(N, -1)

# --- ูุณุชุฎุฏู im2col/col2im ูุชูุฑูุฑ ูุนุงู ููุงุถุญ ---
def _im2col(x, KH, KW, SH, SW, PH, PW):
    N, C, H, W = x.shape
    OH = (H + 2*PH - KH)//SH + 1
    OW = (W + 2*PW - KW)//SW + 1
    x_padded = np.pad(x, ((0,0),(0,0),(PH,PH),(PW,PW)), mode='constant')
    cols = np.zeros((N, C * KH * KW, OH * OW), dtype=x.dtype)
    col_idx = 0
    for i in range(0, H + 2*PH - KH + 1, SH):
        for j in range(0, W + 2*PW - KW + 1, SW):
            patch = x_padded[:, :, i:i+KH, j:j+KW]
            cols[:, :, col_idx] = patch.reshape(N, -1)
            col_idx += 1
    return cols, OH, OW

def _col2im(cols, x_shape, KH, KW, SH, SW, PH, PW):
    N, C, H, W = x_shape
    x_padded = np.zeros((N, C, H + 2*PH, W + 2*PW), dtype=cols.dtype)
    OH = (H + 2*PH - KH)//SH + 1
    OW = (W + 2*PW - KW)//SW + 1
    col_idx = 0
    for i in range(0, H + 2*PH - KH + 1, SH):
        for j in range(0, W + 2*PW - KW + 1, SW):
            patch = cols[:, :, col_idx].reshape(N, C, KH, KW)
            x_padded[:, :, i:i+KH, j:j+KW] += patch
            col_idx += 1
    return x_padded[:, :, PH:PH+H, PW:PW+W]

class Conv2d(Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        self.in_channels, self.out_channels = in_channels, out_channels
        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size
        self.stride = (stride, stride) if isinstance(stride, int) else stride
        self.padding = (padding, padding) if isinstance(padding, int) else padding
        KH, KW = self.kernel_size
        limit = np.sqrt(2 / (in_channels * KH * KW))
        self.weight = Node(np.random.randn(out_channels, in_channels, KH, KW) * limit)
        self.bias = Node(np.zeros(out_channels))

    def forward(self, x):
        # x is Node
        x_arr = x.value
        KH, KW = self.kernel_size
        SH, SW = self.stride
        PH, PW = self.padding
        cols, OH, OW = nn._im2col(x_arr, KH, KW, SH, SW, PH, PW)
        # cols: (N, C*KH*KW, OH*OW)
        N = x_arr.shape[0]
        w_col = self.weight.value.reshape(self.out_channels, -1)  # (O, C*KH*KW)
        out_cols = np.einsum('oc,nck->nok', w_col, cols)  # (O, N, OH*OW)
        out_cols = out_cols.transpose(1,0,2).reshape(N, self.out_channels, OH, OW)
        out_val = out_cols + self.bias.value.reshape(1, -1, 1, 1)
        out = Node(out_val, (x, self.weight, self.bias), 'conv2d')

        def _backward(out_node):
            g = out_node.grad  # (N, O, OH, OW)
            # reshape to (N, O, OH*OW)
            g_cols = g.reshape(N, self.out_channels, -1)
            # weight.grad: sum_n g_cols @ cols.T
            # compute per-sample contributions then sum
            # cols: (N, C*KH*KW, OH*OW)
            # we want weight.grad shape (O, C, KH, KW)
            w_grad = np.zeros_like(self.weight.value)
            for n in range(N):
                # g_cols[n]: (O, OH*OW); cols[n]: (C*KH*KW, OH*OW)
                w_grad += (g_cols[n] @ cols[n].T).reshape(self.weight.value.shape)
            if self.weight.grad is None: self.weight.grad = np.zeros_like(self.weight.value)
            self.weight.grad += w_grad
            # bias grad
            if self.bias.grad is None: self.bias.grad = np.zeros_like(self.bias.value)
            self.bias.grad += g.sum(axis=(0, 2, 3))
            # grad wrt x via cols: (N, C*KH*KW, OH*OW) = w_col.T @ g_cols
            grad_cols = np.einsum('oc,nok->nck', w_col.T, g_cols)
            # reconstruct to image
            x_grad = nn._col2im(grad_cols, x_arr.shape, KH, KW, SH, SW, PH, PW)
            if x.grad is None: x.grad = np.zeros_like(x.value)
            x.grad += x_grad

        out._backward = _backward
        return out

    def parameters(self): yield from [self.weight, self.bias]

class MaxPool2d(Module):
    def __init__(self, kernel_size, stride=None, padding=0):
        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size
        self.stride = (stride, stride) if isinstance(stride, int) else (kernel_size, kernel_size) if stride is None else stride
        self.padding = (padding, padding) if isinstance(padding, int) else padding

    def forward(self, x):
        x_arr = x.value
        N, C, H, W = x_arr.shape
        KH, KW = self.kernel_size
        SH, SW = self.stride
        PH, PW = self.padding
        cols, OH, OW = nn._im2col(x_arr, KH, KW, SH, SW, PH, PW)
        # cols: (N, C*KH*KW, OH*OW)
        cols_reshaped = cols.reshape(N, C, KH*KW, OH*OW)
        # compute max along kernel elements
        max_idx = np.argmax(cols_reshaped, axis=2)  # (N, C, OH*OW)
        out_cols = np.max(cols_reshaped, axis=2)    # (N, C, OH*OW)
        out_val = out_cols.reshape(N, C, OH, OW)
        out = Node(out_val, (x,), 'maxpool2d')

        def _backward(out_node):
            g = out_node.grad.reshape(N, C, -1)  # (N, C, OH*OW)
            grad_cols = np.zeros_like(cols_reshaped)
            idx = max_idx
            # scatter gradients back to the selected positions
            n_idx, c_idx = np.indices(idx.shape[:2])
            for n in range(N):
                for c in range(C):
                    grad_cols[n, c, idx[n, c], np.arange(idx.shape[2])] = g[n, c]
            grad_cols = grad_cols.reshape(N, C*KH*KW, -1)
            x_grad = nn._col2im(grad_cols, x_arr.shape, KH, KW, SH, SW, PH, PW)
            if x.grad is None: x.grad = np.zeros_like(x.value)
            x.grad += x_grad

        out._backward = _backward
        return out

class Sequential(Module):
    def __init__(self, *layers): self.layers = layers
    def forward(self, x):
        for layer in self.layers: x = layer(x)
        return x
    def parameters(self):
        for layer in self.layers: yield from layer.parameters()

==============================================================================

ุงููุณู 3: ุงูููุญุณููู ุงููุจุชูุฑ - AdamOptimizer (ูุณุฎุฉ ููุญุณููุฉ)

==============================================================================

class AdamOptimizer: def init(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, circular_strength=0.0, echo_freq=1, neumann_steps=1, echo_norm_clip=None): self.params = list(params) self.lr, self.eps = lr, eps self.beta1, self.beta2 = betas self.t = 0 self.m = {id(p): np.zeros_like(p.value) for p in self.params} self.v = {id(p): np.zeros_like(p.value) for p in self.params} self.circular_strength = circular_strength self.echo_freq, self.neumann_steps = echo_freq, max(1, neumann_steps) self.echo_norm_clip = echo_norm_clip

def zero_grad(self):
    for p in self.params: p.grad = None

def step(self, topo_graph=None):
    self.t += 1
    for p in self.params:
        p_grad = p.grad if p.grad is not None else np.zeros_like(p.value)
        self.m[id(p)] = self.beta1 * self.m[id(p)] + (1 - self.beta1) * p_grad
        self.v[id(p)] = self.beta2 * self.v[id(p)] + (1 - self.beta2) * (p_grad ** 2)
        m_hat = self.m[id(p)] / (1 - self.beta1 ** self.t)
        v_hat = self.v[id(p)] / (1 - self.beta2 ** self.t)
        p.value -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)
        p.original_grad = p_grad.copy()

    # Circular "echo" propagation
    if self.circular_strength > 0 and topo_graph is not None and (self.t % self.echo_freq == 0):
        last_param = next((p for p in reversed(self.params) if p.value.ndim >= 1), self.params[-1])
        echo = getattr(last_param, 'original_grad', np.zeros_like(last_param.value)).astype(np.float32)
        param_scale = np.sqrt(np.prod(last_param.value.shape)).astype(np.float32)
        echo = echo / (param_scale + 1e-12) * self.circular_strength
        if self.echo_norm_clip is not None:
            norm = np.linalg.norm(echo.ravel())
            if norm > self.echo_norm_clip: echo *= self.echo_norm_clip / (norm + 1e-12)

        for p in self.params: p.grad = np.zeros_like(p.value)
        last_param.grad = echo
        # re-backpropagate (Neumann-style steps)
        for _ in range(self.neumann_steps):
            for v in reversed(topo_graph): v._backward(v)
        # final small update from echoed grads
        for p in self.params:
            if p.grad is not None: p.value -= self.lr * p.grad * self.circular_strength

==============================================================================

SECTION 4: ุฃุฏูุงุช ูุณุงุนุฏุฉ

==============================================================================

def set_seed(seed=42): np.random.seed(seed) try: import random as _r _r.seed(seed) except Exception: pass

def batch_iterator(X, y, batch_size=256, shuffle=True): n = X.shape[0] idx = np.arange(n) if shuffle: np.random.shuffle(idx) for i in range(0, n, batch_size): yield X[idx[i:i+batch_size]], y[idx[i:i+batch_size]]

def init_model_weights(model): for layer in getattr(model, 'layers', []): if isinstance(layer, nn.Linear): limit = np.sqrt(2 / layer.weight.value.shape[0]) layer.weight.value[:] = np.random.randn(*layer.weight.value.shape) * limit layer.bias.value[:] = 0 elif isinstance(layer, nn.Conv2d): KH, KW = layer.kernel_size in_ch = layer.weight.value.shape[1] limit = np.sqrt(2 / (in_ch * KH * KW)) layer.weight.value[:] = np.random.randn(*layer.weight.value.shape) * limit layer.bias.value[:] = 0

def cross_entropy_loss_stable(logits_node, labels_numpy): # logits_node: Node with shape (N, C) N, C = logits_node.value.shape # stable softmax cross-entropy using Node ops where possible max_logits = Node(np.max(logits_node.value, axis=1, keepdims=True)) shifted = logits_node - max_logits log_sum_exp = (shifted.exp().sum(axis=1, keepdims=True)).log() log_probs = shifted - log_sum_exp one_hot = np.zeros((N, C), dtype=np.float32) one_hot[np.arange(N), labels_numpy] = 1.0 loss = -(Node(one_hot) * log_probs).sum() * (1.0 / float(N)) return loss

==============================================================================

ุงููุณู 5: ุงูุชุฌุฑุจุฉ ุงูุญุงุณูุฉ

==============================================================================

def run_final_experiment(model_factory, X_train, y_train, X_test, y_test, config, experiment_name): print(f"\n๐๐๐ ุจุฏุก ุงูุชุฌุฑุจุฉ ุงูุญุงุณูุฉ: {experiment_name} ๐๐๐") print(f"ุงูุชูููู: {config}") model = model_factory() init_model_weights(model) optimizer = AdamOptimizer(model.parameters(), **config['optimizer_params']) for epoch in range(1, config['epochs'] + 1): epoch_start_time = time.time() model.train() epoch_loss = 0.0 seen = 0 for xb, yb in batch_iterator(X_train, y_train, batch_size=config['batch_size']): x_node = Node(xb) logits = model(x_node) loss = cross_entropy_loss_stable(logits, yb) model.zero_grad(); optimizer.zero_grad() topo = loss.backward() optimizer.step(topo_graph=topo) epoch_loss += float(loss.value) * len(xb) seen += len(xb) epoch_loss /= seen epoch_duration = time.time() - epoch_start_time model.eval() # evaluation val_preds_all = [] for xb_val, _ in batch_iterator(X_test, y_test, batch_size=config['batch_size'], shuffle=False): val_logits = model(Node(xb_val)) val_preds_all.append(np.argmax(val_logits.value, axis=1)) val_preds = np.concatenate(val_preds_all) val_acc = np.mean(val_preds == y_test) print(f"--- Epoch {epoch}/{config['epochs']} --- " f"Train Loss: {epoch_loss:.4f} | Val Accuracy: {val_acc:.4f} | Time: {epoch_duration:.2f}s ---") return val_acc

==============================================================================

ููุทุฉ ุงูุชุดุบูู

==============================================================================

if name == 'main': print("1. ุชุญููู ููุนุงูุฌุฉ ุจูุงูุงุช MNIST ุงููุงููุฉ...") (X_train_full, y_train_full), (X_test, y_test) = mnist.load_data() X_train_full = X_train_full.reshape(-1, 1, 28, 28).astype(np.float32) / 255.0 X_test = X_test.reshape(-1, 1, 28, 28).astype(np.float32) / 255.0 print(f"ุชู ุชุฌููุฒ ุงูุจูุงูุงุช. ุญุฌู ุงูุชุฏุฑูุจ: {X_train_full.shape}, ุญุฌู ุงูุงุฎุชุจุงุฑ: {X_test.shape}")

def cnn_model_factory():
    return nn.Sequential(
        nn.Conv2d(1, 8, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2, 2),
        nn.Conv2d(8, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),
        nn.Flatten(), nn.Linear(16 * 7 * 7, 128), nn.ReLU(), nn.Linear(128, 10)
    )
print("2. ุชู ุชุนุฑูู ูุตูุน ููุงุฐุฌ CNN.")

common_config = {'epochs': 8, 'batch_size': 256}
baseline_config = {**common_config, 'optimizer_params': {'lr': 5e-4, 'circular_strength': 0.0}}
circular_config = {**common_config, 'optimizer_params': {'lr': 5e-4, 'circular_strength': 0.01, 'neumann_steps': 3, 'echo_freq': 1, 'echo_norm_clip': 3.0}}

set_seed(42)
baseline_accuracy = run_final_experiment(cnn_model_factory, X_train_full, y_train_full, X_test, y_test, baseline_config, "Baseline_CNN")
set_seed(42)
circular_accuracy = run_final_experiment(cnn_model_factory, X_train_full, y_train_full, X_test, y_test, circular_config, "Circular_Prop_CNN")

print("\n\n========================================")
print("          ๐ ุงููุชุงุฆุฌ ุงูููุงุฆูุฉ ๐")
print("========================================")
print(f"ุงูุฏูุฉ ุงูููุงุฆูุฉ ููุฎุท ุงูุฃุณุงุณู (Baseline): {baseline_accuracy * 100:.2f}%")
print(f"ุงูุฏูุฉ ุงูููุงุฆูุฉ ููุงูุชุดุงุฑ ุงูุฏุงุฆุฑู:       {circular_accuracy * 100:.2f}%")
print("----------------------------------------")
improvement = circular_accuracy - baseline_accuracy
if improvement > 0.0005:
    print(f"๐ ูุฌุงุญ! ุงูุงูุชุดุงุฑ ุงูุฏุงุฆุฑู ุฃุฏู ุฅูู ุชุญุณู ูู ุงูุฏูุฉ ุจููุฏุงุฑ {improvement * 100:.2f}%")
elif improvement < -0.0005:
    print(f"โ๏ธ ุงูุชุจุงู! ุงูุงูุชุดุงุฑ ุงูุฏุงุฆุฑู ุฃุฏู ุฅูู ุชุฑุงุฌุน ูู ุงูุฏูุฉ ุจููุฏุงุฑ {improvement * 100:.2f}%")
else:
    print("โน๏ธ ูุง ูุฑู ูุนููู ุจูู ุงูุทุฑููุชูู (ุฏุงุฎู ูุงูุด ุงูุถูุถุงุก).")

