import numpy as np
import time
import random
import matplotlib.pyplot as plt
import json

# ==============================================================================
# القسم 1: المحرك الأساسي (Node, nn) والوظائف المساعدة - النسخة النهائية
# ==============================================================================
# ... (الكود الكامل والمستقر لفئة Node وجميع فئات nn مثل Linear, PReLU, Tanh, Sequential)
# ... (الكود الكامل لدوال استراتيجيات معدل التعلم مثل constant_lr, linear_decay_lr, etc.)
# ... (الكود الكامل لدالة create_dynamic_model)
def _sum_to_shape(grad, shape):
    while grad.ndim > len(shape): grad = grad.sum(axis=0)
    for i, (g_dim, t_dim) in enumerate(zip(grad.shape, shape)):
        if t_dim == 1 and g_dim != 1: grad = grad.sum(axis=i, keepdims=True)
    return grad.reshape(shape)
class Node:
    def __init__(self, value, parents=(), op=''):
        self.value = np.array(value, dtype=float); self.parents = parents; self.op = op
        self.grad = None; self._backward = lambda: None
    def _ensure(self, other):
        return other if isinstance(other, Node) else Node(np.array(other, dtype=float))
    def __add__(self, other):
        other = self._ensure(other); out = Node(self.value + other.value, (self, other), '+')
        def _backward():
            if out.grad is None: return
            self.grad += _sum_to_shape(out.grad, self.value.shape)
            other.grad += _sum_to_shape(out.grad, other.value.shape)
        out._backward = _backward; return out
    def __mul__(self, other):
        other = self._ensure(other); out = Node(self.value * other.value, (self, other), '*')
        def _backward():
            if out.grad is None: return
            self.grad += _sum_to_shape(out.grad * other.value, self.value.shape)
            other.grad += _sum_to_shape(out.grad * self.value, other.value.shape)
        out._backward = _backward; return out
    def __matmul__(self, other):
        other = self._ensure(other); out = Node(self.value @ other.value, (self, other), '@')
        def _backward():
            if out.grad is None: return
            A, B, G = self.value, other.value, out.grad
            self.grad += _sum_to_shape(G @ B.T, self.value.shape)
            other.grad += _sum_to_shape(A.T @ G, other.value.shape)
        out._backward = _backward; return out
    def sum(self):
        out = Node(self.value.sum(), (self,), 'sum')
        def _backward():
            if out.grad is None: return
            self.grad += np.broadcast_to(out.grad, self.value.shape)
        out._backward = _backward; return out
    def backward(self):
        topo, visited = [], set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for p in v.parents: build_topo(p)
                topo.append(v)
        build_topo(self)
        for v in topo: v.grad = np.zeros_like(v.value)
        self.grad = np.ones_like(self.value)
        for v in reversed(topo): v._backward()

class nn:
    class Module:
        def parameters(self): yield from []
        def __call__(self, *args, **kwargs): return self.forward(*args, **kwargs)
        def zero_grad(self):
            for p in self.parameters(): p.grad = np.zeros_like(p.value)
    class Linear(Module):
        def __init__(self, in_features, out_features):
            self.in_features = in_features
            limit = np.sqrt(1 / self.in_features)
            self.weight = Node(np.random.randn(in_features, out_features) * limit)
            self.bias = Node(np.zeros(out_features))
        def forward(self, x): return x @ self.weight + self.bias
        def parameters(self): yield from [self.weight, self.bias]
    class PReLU(Module):
        def __init__(self, initial_alpha=0.01):
            self.alpha = Node(np.array([initial_alpha]))
        def forward(self, x):
            positive_part = Node(np.maximum(0, x.value)); negative_part = Node(np.minimum(0, x.value))
            out = positive_part + (negative_part * self.alpha); out.parents = (x, self.alpha); out.op = 'prelu'
            def _backward():
                if out.grad is None: return
                grad_x = (x.value > 0) * 1.0 + (x.value <= 0) * self.alpha.value
                x.grad += grad_x * out.grad
                grad_alpha = (x.value * (x.value <= 0)) * out.grad
                self.alpha.grad += grad_alpha.sum()
            out._backward = _backward; return out
        def parameters(self): yield self.alpha
    class Tanh(Module):
        def forward(self, x):
            out = Node(np.tanh(x.value), parents=(x,), op='tanh')
            def _backward():
                if out.grad is None: return
                x.grad += (1 - out.value**2) * out.grad
            out._backward = _backward; return out
    class Sequential(Module):
        def __init__(self, *layers): self.layers = layers
        def forward(self, x):
            for layer in self.layers: x = layer(x)
            return x
        def parameters(self):
            for layer in self.layers: yield from layer.parameters()

def constant_lr(epoch, initial_lr, **kwargs): return initial_lr
def linear_decay_lr(epoch, initial_lr, total_epochs, **kwargs): return initial_lr * (1 - (epoch / total_epochs))
def exponential_decay_lr(epoch, initial_lr, decay_rate, **kwargs): return initial_lr * (decay_rate ** epoch)
def cyclical_lr(epoch, initial_lr, max_lr, step_size, **kwargs):
    cycle = np.floor(1 + epoch / (2 * step_size)); x = np.abs(epoch / step_size - 2 * cycle + 1)
    return initial_lr + (max_lr - initial_lr) * np.maximum(0, (1 - x))

LR_SCHEDULER_REGISTRY = {'constant': constant_lr, 'linear': linear_decay_lr, 'exponential': exponential_decay_lr, 'cyclical': cyclical_lr}

def create_dynamic_model(config):
    H1 = config.get('H1', 64); H2 = config.get('H2', 64)
    activation1_type = config.get('activation1', 'prelu'); activation2_type = config.get('activation2', 'prelu')
    np.random.seed(config.get('seed', 42))
    act1 = nn.Tanh() if activation1_type == 'tanh' else nn.PReLU()
    act2 = nn.Tanh() if activation2_type == 'tanh' else nn.PReLU()
    return nn.Sequential(nn.Linear(1, H1), act1, nn.Linear(H1, H2), act2, nn.Linear(H2, 1))

# ==============================================================================
# القسم 2: الأوركسترا النهائي ودواله المساعدة
# ==============================================================================

def create_ultimate_genome():
    """
    ينشئ جينومًا كاملاً يصف البنية، والتنشيط، واستراتيجية التعلم.
    """
    config = {
        'H1': random.choice([16, 32, 64]), 'H2': random.choice([16, 32, 64]),
        'activation1': random.choice(['prelu', 'tanh']), 'activation2': random.choice(['prelu', 'tanh']),
        'seed': random.randint(0, 10000)
    }
    lr_strategy_type = random.choice(['constant', 'linear', 'exponential', 'cyclical'])
    lr_params = {'type': lr_strategy_type, 'initial_lr': 10**random.uniform(-3, -1.5)}
    if lr_strategy_type == 'exponential':
        lr_params['decay_rate'] = random.uniform(0.99, 0.999)
    elif lr_strategy_type == 'cyclical':
        lr_params['max_lr'] = lr_params['initial_lr'] * random.uniform(2, 5)
        lr_params['step_size'] = random.choice([150, 250, 350])
    config['lr_strategy'] = lr_params
    return config

def run_ultimate_trial(config, X_train, y_train, X_test, y_test):
    """
    تجري تجربة واحدة كاملة باستخدام التكوين الشامل من الجينوم.
    """
    lr_config = config['lr_strategy']
    config_str = (f"H1={config['H1']}, Act1={config['activation1']}, H2={config['H2']}, Act2={config['activation2']}, "
                  f"LR_Strat={lr_config['type']}, Init_LR={lr_config['initial_lr']:.4f}")
    print(f"\n--- 🧪 اختبار الجينوم: {config_str} ---")

    model = create_dynamic_model(config)
    params = list(model.parameters())
    adam_state = [{'m': np.zeros_like(p.value), 'v': np.zeros_like(p.value)} for p in params]
    beta1, beta2, eps = 0.9, 0.999, 1e-8
    epochs = 1000
    weight_decay = 1e-4
    
    scheduler_fn = LR_SCHEDULER_REGISTRY.get(lr_config['type'], constant_lr)
    scheduler_args = {**lr_config, 'total_epochs': epochs}

    for epoch in range(1, epochs + 1):
        current_lr = scheduler_fn(epoch, **scheduler_args)
        y_pred = model(X_train)
        loss = ((y_pred + (y_train * -1)) * (y_pred + (y_train * -1))).sum()
        model.zero_grad()
        loss.backward()
        for i, p in enumerate(params):
            p.value -= weight_decay * p.value
            adam_state[i]['m'] = beta1 * adam_state[i]['m'] + (1 - beta1) * p.grad
            adam_state[i]['v'] = beta2 * adam_state[i]['v'] + (1 - beta2) * (p.grad**2)
            m_hat = adam_state[i]['m'] / (1 - beta1**epoch)
            v_hat = adam_state[i]['v'] / (1 - beta2**epoch)
            p.value -= current_lr * m_hat / (np.sqrt(v_hat) + eps)
            
    test_pred = model(X_test)
    final_loss = np.mean((test_pred.value - y_test.value)**2)
    print(f"-> الخسارة النهائية: {final_loss:.5f}")
    return final_loss

def ultimate_orchestrator(n_trials=50):
    """
    ينسق عملية البحث الشاملة.
    """
    print("🎼 إطلاق الأوركسترا النهائي (v4 - تطور كل شيء!) 🎼")
    
    np.random.seed(42)
    X_data = np.linspace(-5, 5, 100)[:, np.newaxis]
    y_data = np.sin(X_data) + np.cos(X_data * 0.5) + np.random.randn(100, 1) * 0.2
    train_indices = np.random.choice(100, 80, replace=False)
    test_indices = np.setdiff1d(np.arange(100), train_indices)
    X_train, y_train = Node(X_data[train_indices]), Node(y_data[train_indices])
    X_test, y_test = Node(X_data[test_indices]), Node(y_data[test_indices])

    best_config = {}
    best_loss = float('inf')
    log = []

    for i in range(n_trials):
        print(f"\n--- [التجربة الكبرى {i+1}/{n_trials}] ---")
        candidate_config = create_ultimate_genome()
        final_loss = run_ultimate_trial(candidate_config, X_train, y_train, X_test, y_test)
        log.append({'config': candidate_config, 'loss': final_loss})
        if final_loss < best_loss:
            best_loss = final_loss
            best_config = candidate_config
            print(f"🏆 *** أفضل نتيجة شاملة جديدة! ***")

    print("\n\n--- 🏁 اكتمل تطور الأوركسترا النهائي 🏁 ---")
    print("أفضل تكوين شامل تم العثور عليه:")
    # استخدام json.dumps للطباعة المنظمة
    print(json.dumps(best_config, indent=2))
    print(f"\n  أقل خسارة اختبار محققة: {best_loss:.5f}")
    
    return best_config, log

# ==============================================================================
# القسم 3: التنفيذ
# ==============================================================================
if __name__ == '__main__':
    start_time = time.time()
    best_config, full_log = ultimate_orchestrator(n_trials=50)
    end_time = time.time()
    print(f"\nإجمالي وقت البحث: {end_time - start_time:.2f} ثانية.")
